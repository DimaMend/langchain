{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce8457ed-c0b1-4a74-abbd-9d3d2211270f",
   "metadata": {},
   "source": [
    "# # Migrating off ConversationBufferWindowMemory or ConversationTokenBufferMemory\n",
    "\n",
    "Follow this guide if you're trying to migrate off one of the old memory classes listed below:\n",
    "\n",
    "\n",
    "| Memory Type                          | Description                                                                                                                                          |\n",
    "|---------------------------------------|-----------------------------------------------------------------------\n",
    "| `ConversationBufferWindowMemory`      | Keeps the last `n` turns of the conversation. Drops the oldest turn when the buffer is full.                                                          |\n",
    "| `ConversationTokenBufferMemory`       | Keeps only the most recent messages in the conversation under the constraint that the total number of tokens in the conversation does not exceed a certain limit. |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7480a897-eebc-48c0-8cf8-19bf84c27de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "</details>\n",
    "\n",
    "## Implementing Conversation History Processing\n",
    "\n",
    "Each of the following memory types applies specific logic to handle the conversation history:\n",
    "\n",
    "| Memory Type                          | Description                                                                                                                                          |\n",
    "|---------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| `ConversationBufferWindowMemory`      | Keeps the last `n` turns of the conversation. Drops the oldest turn when the buffer is full.                                                          |\n",
    "| `ConversationTokenBufferMemory`       | Keeps only the most recent messages in the conversation under the constraint that the total number of tokens in the conversation does not exceed a certain limit. |\n",
    "| `ConversationSummaryMemory`           | Continually summarizes the conversation history. The summary is updated after each conversation turn. The abstraction returns the summary of the conversation history. |\n",
    "| `ConversationSummaryBufferMemory`     | Provides a running summary of the conversation together with the most recent messages in the conversation under the constraint that the total number of tokens in the conversation does not exceed a certain limit. |\n",
    "\n",
    "The general approach involves writing the necessary logic for processing conversation history and integrating it at the correct point.\n",
    "\n",
    "We’ll start by building a simple processor using LangChain's built-in [trim_messages](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.utils.trim_messages.html) function, and then demonstrate how to integrate it into your application.\n",
    "\n",
    "You can later replace this basic setup with more advanced logic tailored to your specific needs.\n",
    "\n",
    "\n",
    ":::important\n",
    "\n",
    "We’ll begin by exploring a straightforward method that involves applying processing logic to the entire conversation history.\n",
    "\n",
    "While this approach is easy to test and implement, it has a downside: as the conversation grows, s\n",
    "o does the latency, since the logic is re-applied to all previous exchanges at each turn.\n",
    "\n",
    "More advanced strategies focus on incrementally updating the conversation history to avoid redundant processing.\n",
    "\n",
    "For instance, the langgraph [how-to guide on summarization](https://langchain-ai.github.io/langgraph/how-tos/memory/add-summary-conversation-history/) demonstrates\n",
    "how to maintain a running summary of the conversation while discarding older messages, ensuring they aren't re-processed during later turns.\n",
    ":::\n",
    "\n",
    "\n",
    "### ConversationBufferWindowMemory, ConversationTokenBufferMemory\n",
    "\n",
    "<details open>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07f9459-9fb6-4942-99c9-64558aedd7d4",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b99b47ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --upgrade --quiet langchain-openai langchain langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "717c8673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0f5921-cc51-4f08-9364-a2eab0ad8b3d",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "## Implementing Conversation History Processing\n",
    "\n",
    "Each of the following memory types applies specific logic to handle the conversation history:\n",
    "\n",
    "| Memory Type                          | Description                                                                                                                                          |\n",
    "|---------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| `ConversationBufferWindowMemory`      | Keeps the last `n` turns of the conversation. Drops the oldest turn when the buffer is full.                                                          |\n",
    "| `ConversationTokenBufferMemory`       | Keeps only the most recent messages in the conversation under the constraint that the total number of tokens in the conversation does not exceed a certain limit. |\n",
    "| `ConversationSummaryMemory`           | Continually summarizes the conversation history. The summary is updated after each conversation turn. The abstraction returns the summary of the conversation history. |\n",
    "| `ConversationSummaryBufferMemory`     | Provides a running summary of the conversation together with the most recent messages in the conversation under the constraint that the total number of tokens in the conversation does not exceed a certain limit. |\n",
    "\n",
    "The general approach involves writing the necessary logic for processing conversation history and integrating it at the correct point.\n",
    "\n",
    "We’ll start by building a simple processor using LangChain's built-in [trim_messages](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.utils.trim_messages.html) function, and then demonstrate how to integrate it into your application.\n",
    "\n",
    "You can later replace this basic setup with more advanced logic tailored to your specific needs.\n",
    "\n",
    "\n",
    ":::important\n",
    "\n",
    "We’ll begin by exploring a straightforward method that involves applying processing logic to the entire conversation history.\n",
    "\n",
    "While this approach is easy to test and implement, it has a downside: as the conversation grows, s\n",
    "o does the latency, since the logic is re-applied to all previous exchanges at each turn.\n",
    "\n",
    "More advanced strategies focus on incrementally updating the conversation history to avoid redundant processing.\n",
    "\n",
    "For instance, the langgraph [how-to guide on summarization](https://langchain-ai.github.io/langgraph/how-tos/memory/add-summary-conversation-history/) demonstrates\n",
    "how to maintain a running summary of the conversation while discarding older messages, ensuring they aren't re-processed during later turns.\n",
    ":::\n",
    "\n",
    "\n",
    "### ConversationBufferWindowMemory, ConversationTokenBufferMemory\n",
    "\n",
    "<details open>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a501806e-deac-458c-88b1-b615efde9930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant, you always respond with a joke.\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Hmmm let me think.\\n\\nWhy, he's probably chasing after the last cup of coffee in the office!\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='why is 42 always the answer?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Because it’s the only number that’s constantly right, even when it doesn’t add up!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What did the cow say?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    BaseMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    trim_messages,\n",
    ")\n",
    "\n",
    "\n",
    "full_message_history = [\n",
    "    SystemMessage(\"you're a good assistant, you always respond with a joke.\"),\n",
    "    HumanMessage(\"i wonder why it's called langchain\"),\n",
    "    AIMessage(\n",
    "        'Well, I guess they thought \"WordRope\" and \"SentenceString\" just didn\\'t have the same ring to it!'\n",
    "    ),\n",
    "    HumanMessage(\"and who is harrison chasing anyways\"),\n",
    "    AIMessage(\n",
    "        \"Hmmm let me think.\\n\\nWhy, he's probably chasing after the last cup of coffee in the office!\"\n",
    "    ),\n",
    "    HumanMessage(\"why is 42 always the answer?\"),\n",
    "    AIMessage(\n",
    "        \"Because it’s the only number that’s constantly right, even when it doesn’t add up!\"\n",
    "    ),\n",
    "    HumanMessage(\"What did the cow say?\"),\n",
    "]\n",
    "\n",
    "\n",
    "def message_processor(messages: list[BaseMessage]) -> list[BaseMessage]:\n",
    "    \"\"\"A sample message processor that:\n",
    "\n",
    "    1. Keeps the system message\n",
    "    2. Keeps up to max number of messages\n",
    "    3. Make sure that the last message is a HumanMessage\n",
    "\n",
    "    You will likely want to instead count based on tokens,\n",
    "    and/or increase the number of messages.\n",
    "\n",
    "    Please see the API reference for trim_messages for more details.\n",
    "\n",
    "    https://python.langchain.com/api_reference/core/messages/langchain_core.messages.utils.trim_messages.html\n",
    "    \"\"\"\n",
    "    return trim_messages(\n",
    "        messages,\n",
    "        token_counter=len,  # <-- Will just count the number of messages rather than tokens\n",
    "        max_tokens=5,  # <-- allow up to 5 messages.\n",
    "        strategy=\"last\",\n",
    "        include_system=True,\n",
    "        allow_partial=False,\n",
    "    )\n",
    "\n",
    "\n",
    "message_processor(full_message_history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7edbe5ab-f9cc-4ae1-b4ac-9ee82c1094ef",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "## LCEL: Add a pre-processor in front of the chat model\n",
    "\n",
    "The simplest way to add complex conversation management is by introducing a pre-processing step in front of the chat model and pass the full conversation history to the pre-processing step.\n",
    "\n",
    "This approach is conceptually simple and will work in many situations; for example, if using a [RunnableWithMessageHistory](/docs/how_to/message_history/) instead of wrapping the chat model, wrap the chat model with the pre-processor.\n",
    "\n",
    "The obvious downside of this approach is that latency starts to increase as the conversation history grows because of two reasons:\n",
    "\n",
    "1. As the conversation gets longer, more data may need to be fetched from whatever store your'e using to store the conversation history (if not storing it in memory).\n",
    "2. The pre-processing logic will end up doing a lot of redundant computation, repeating computation from previous steps of the conversation.\n",
    "\n",
    ":::caution\n",
    "\n",
    "If you're using tools, remember to bind the tools to the model before adding a pre-processing step to it!\n",
    "\n",
    ":::\n",
    "\n",
    "<details open>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5537b001-a49c-4d12-b25d-4087890f49b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_g7l7AFBHnfInA9ps2DpnkrsU', 'function': {'arguments': '{}', 'name': 'what_did_the_cow_say'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 126, 'total_tokens': 142, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-3df002ca-da48-4b91-ad7f-c8e3ad5c7550-0', tool_calls=[{'name': 'what_did_the_cow_say', 'args': {}, 'id': 'call_g7l7AFBHnfInA9ps2DpnkrsU', 'type': 'tool_call'}], usage_metadata={'input_tokens': 126, 'output_tokens': 16, 'total_tokens': 142})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "\n",
    "@tool\n",
    "def what_did_the_cow_say() -> str:\n",
    "    \"\"\"Check to see what the cow said.\"\"\"\n",
    "    return \"foo\"\n",
    "\n",
    "model_with_tools = model.bind_tools([what_did_the_cow_say])\n",
    "\n",
    "# highlight-next-line\n",
    "model_with_preprocessor = message_processor | model_with_tools\n",
    "\n",
    "# full_message_history in the previous code block.\n",
    "# We pass it explicity to the model_with_preprocesor for illustrative purposes.\n",
    "# If you're using `RunnableWithMessageHistory` the history will be automatically\n",
    "# read from the source the you configure.\n",
    "model_with_preprocessor.invoke(full_message_history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2edf6e21-159c-4bb1-bae0-5d094c68ed4c",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "If you need to implement more efficient logic and want to use `RunnableWithMessageHistory` for now the way to achieve this\n",
    "is to subclass from [BaseChatMessageHistory](https://api.python.langchain.com/en/latest/chat_history/langchain_core.chat_history.BaseChatMessageHistory.html) and\n",
    "define appropriate logic for `add_messages` (that doesn't simply append the history, but instead re-writes it).\n",
    "\n",
    "Unless you have a good reason to implement this solution, you should instead use LangGraph,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84229e2e-a578-4b21-840a-814223406402",
   "metadata": {},
   "source": [
    "## LangGraph\n",
    "\n",
    "### Agent Executor with a pre-built agent\n",
    "\n",
    "If you're migrating off a pre-built langchain agent that uses memory.\n",
    "\n",
    "You can create a pre-built agent using: [create_react_agent](https://langchain-ai.github.io/langgraph/reference/prebuilt/#create_react_agent).\n",
    "\n",
    "To add memory pre-processing to the agent, you can do the following:\n",
    "\n",
    "```python\n",
    "\n",
    "...\n",
    "\n",
    "# highlight-start\n",
    "def state_modifier(state) -> list[BaseMessage]:\n",
    "    \"\"\"Given the agent state, return a list of messages for the chat model.\"\"\"\n",
    "    # We're using the message processor defined above.\n",
    "    return message_processor(state['messages'])\n",
    "# highlight-end    \n",
    "\n",
    "app = create_react_agent(\n",
    "    model,\n",
    "    tools=[get_user_age], \n",
    "    checkpointer=memory,\n",
    "    # highlight-next-line\n",
    "    state_modifier=state_modifier\n",
    ")\n",
    "\n",
    "...\n",
    "\n",
    "```\n",
    "\n",
    "At each turn of the conversation, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f671db87-8f01-453e-81fd-4e603140a512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "hi! I'm bob. What is my age?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  get_user_age (call_kTEpBUbRFbKE3DZolG9tFrgD)\n",
      " Call ID: call_kTEpBUbRFbKE3DZolG9tFrgD\n",
      "  Args:\n",
      "    name: bob\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: get_user_age\n",
      "\n",
      "42 years old\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Bob, you are 42 years old.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "do you remember my name?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Yes, your name is Bob.\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_user_age(name: str) -> str:\n",
    "    \"\"\"Use this tool to find the user's age.\"\"\"\n",
    "    # This is a placeholder for the actual implementation\n",
    "    if \"bob\" in name.lower():\n",
    "        return \"42 years old\"\n",
    "    return \"41 years old\"\n",
    "\n",
    "\n",
    "memory = MemorySaver()\n",
    "model = ChatOpenAI()\n",
    "\n",
    "\n",
    "# highlight-start\n",
    "def state_modifier(state) -> list[BaseMessage]:\n",
    "    \"\"\"Given the agent state, return a list of messages for the chat model.\"\"\"\n",
    "    # We're using the message processor defined above.\n",
    "    return message_processor(state[\"messages\"])\n",
    "\n",
    "\n",
    "# highlight-end\n",
    "\n",
    "app = create_react_agent(\n",
    "    model,\n",
    "    tools=[get_user_age],\n",
    "    checkpointer=memory,\n",
    "    # highlight-next-line\n",
    "    state_modifier=state_modifier,\n",
    ")\n",
    "\n",
    "# The thread id is a unique key that identifies\n",
    "# this particular conversation.\n",
    "# We'll just generate a random uuid here.\n",
    "thread_id = uuid.uuid4()\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "# Tell the AI that our name is Bob, and ask it to use a tool to confirm\n",
    "# that it's capable of working like an agent.\n",
    "input_message = HumanMessage(content=\"hi! I'm bob. What is my age?\")\n",
    "\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()\n",
    "\n",
    "# Confirm that the chat bot has access to previous conversation\n",
    "# and can respond to the user saying that the user's name is Bob.\n",
    "input_message = HumanMessage(content=\"do you remember my name?\")\n",
    "\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a231ade-9f33-4ff8-9a62-20be3fc159b6",
   "metadata": {},
   "source": [
    "### ConversationSummaryMemory / ConversationSummaryBufferMemory\n",
    "\n",
    "It’s essential to summarize conversations efficiently to prevent growing latency as the conversation history grows.\n",
    "\n",
    "Please follow the guide [how to add summary of the conversation history](https://langchain-ai.github.io/langgraph/how-tos/memory/add-summary-conversation-history/) to see learn how to\n",
    "handle conversation summarization efficiently with LangGraph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2717810",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "Explore persistence with LangGraph:\n",
    "\n",
    "* [LangGraph quickstart tutorial](https://langchain-ai.github.io/langgraph/tutorials/introduction/)\n",
    "* [How to add persistence (\"memory\") to your graph](https://langchain-ai.github.io/langgraph/how-tos/persistence/)\n",
    "* [How to manage conversation history](https://langchain-ai.github.io/langgraph/how-tos/memory/manage-conversation-history/)\n",
    "* [How to add summary of the conversation history](https://langchain-ai.github.io/langgraph/how-tos/memory/add-summary-conversation-history/)\n",
    "\n",
    "Add persistence with simple LCEL (favor langgraph for more complex use cases):\n",
    "\n",
    "* [How to add message history](/docs/how_to/message_history/)\n",
    "\n",
    "Working with message history:\n",
    "\n",
    "* [How to trim messages](/docs/how_to/trim_messages)\n",
    "* [How to filter messages](/docs/how_to/filter_messages/)\n",
    "* [How to merge message runs](/docs/how_to/merge_message_runs/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
