{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce8457ed-c0b1-4a74-abbd-9d3d2211270f",
   "metadata": {},
   "source": [
    "# Migrating off ConversationBufferMemory or ConversationStringBufferMemory\n",
    "\n",
    "[ConversationBufferMemory](https://api.python.langchain.com/en/latest/memory/langchain.memory.buffer.ConversationBufferMemory.html)\n",
    "and [ConversationStringBufferMemory](https://python.langchain.com/api_reference/langchain/memory/langchain.memory.buffer.ConversationStringBufferMemory.html)\n",
    " were used to keep track of a conversation between a human and an ai asstistant without any additional processing. \n",
    "\n",
    "\n",
    ":::note\n",
    "The `ConversationStringBufferMemory` is equivalent to `ConversationBufferMemory` but was targeting LLMs that were not chat models.\n",
    ":::\n",
    "\n",
    "The methods for handling conversation history using existing modern primitives are:\n",
    "\n",
    "1. Using [LangGraph persistence](https://langchain-ai.github.io/langgraph/how-tos/persistence/) along with appropriate processing of the message history\n",
    "2. Using LCEL with [RunnableWithMessageHistory](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) combined with appropriate processing of the message history.\n",
    "\n",
    "Most users will find [LangGraph persistence](https://langchain-ai.github.io/langgraph/how-tos/persistence/) both easier to use and easier to configure than the equivalent LCEL, especially for more complex use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07f9459-9fb6-4942-99c9-64558aedd7d4",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b99b47ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --upgrade --quiet langchain-openai langchain langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "717c8673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3621b62-a037-42b8-8faa-59575608bb8b",
   "metadata": {},
   "source": [
    "## Usage with LLMChain / ConversationChain\n",
    "\n",
    "This section shows how to migrate off `ConversationBufferMemory` or `ConversationStringBufferMemory` that's used together with either an `LLMChain` or a `ConversationChain`.\n",
    "\n",
    "### Legacy\n",
    "\n",
    "Below is example usage of `ConversationBufferMemory` with an `LLMChain` or an equivalent `ConversationChain`.\n",
    "\n",
    "<details open>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b6e1063-cf3a-456a-bf7d-830e5c1d2864",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2668438/1840360377.py:24: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  legacy_chain = LLMChain(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Nice to meet you, Bob! How can I assist you today?', 'chat_history': [HumanMessage(content='my name is bob', additional_kwargs={}, response_metadata={}), AIMessage(content='Nice to meet you, Bob! How can I assist you today?', additional_kwargs={}, response_metadata={})]}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# highlight-start\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "# highlight-end\n",
    "\n",
    "legacy_chain = LLMChain(\n",
    "    llm=ChatOpenAI(),\n",
    "    prompt=prompt,\n",
    "    # highlight-next-line\n",
    "    memory=memory,\n",
    ")\n",
    "\n",
    "legacy_result = legacy_chain.invoke({\"text\": \"my name is bob\"})\n",
    "print(legacy_result)\n",
    "\n",
    "legacy_result = legacy_chain.invoke({\"text\": \"what was my name\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7fa1618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Bob. How can I help you, Bob?'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "legacy_result[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc3b527-c09e-4c77-9711-c3cc4506cd95",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "### LangGraph\n",
    "\n",
    "The example below shows how to use LangGraph to implement a `ConversationChain` or `LLMChain` with `ConversationBufferMemory`.\n",
    "\n",
    "This example assumes that you're already somewhat familiar with `LangGraph`. If you're not, then please see the [LangGraph Quickstart Guide](https://langchain-ai.github.io/langgraph/tutorials/introduction/) for more details.\n",
    "\n",
    "`LangGraph` offers a lot of additional functionality (e.g., time-travel and interrupts) and will work well for other more complex (and realistic) architectures.\n",
    "\n",
    "<details open>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e591965c-c4d7-4df7-966d-4d14bd46e157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "hi! I'm bob\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello Bob! How can I assist you today?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "what was my name?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Bob. How can I assist you, Bob?\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "from IPython.display import Image, display\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "# Define a chat model\n",
    "model = ChatOpenAI()\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define the two nodes we will cycle between\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "\n",
    "# Adding memory is straight forward in langgraph!\n",
    "# highlight-next-line\n",
    "memory = MemorySaver()\n",
    "\n",
    "app = workflow.compile(\n",
    "    # highlight-next-line\n",
    "    checkpointer=memory\n",
    ")\n",
    "\n",
    "\n",
    "# highlight-start\n",
    "# The thread id is a unique key that identifies\n",
    "# this particular conversation.\n",
    "# We'll just generate a random uuid here.\n",
    "thread_id = uuid.uuid4()\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "# highlight-end\n",
    "\n",
    "input_message = HumanMessage(content=\"hi! I'm bob\")\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()\n",
    "\n",
    "# Here, let's confirm that the AI remembers our name!\n",
    "input_message = HumanMessage(content=\"what was my name?\")\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9893029f-43f3-4703-89bf-e0e8fa18aff3",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "### LCEL RunnableWithMessageHistory\n",
    "\n",
    "Alternatively, if you have a simple chain, you can wrap the chat model of the chain within a [RunnableWithMessageHistory](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html).\n",
    "\n",
    "Please refer to [this how to guide](/docs/how_to/message_history/) for more information.\n",
    "\n",
    "## Usasge with a pre-built agent\n",
    "\n",
    "This example shows usage of an Agent Executor with a pre-built agent constructed using the [create_tool_calling_agent](https://api.python.langchain.com/en/latest/agents/langchain.agents.tool_calling_agent.base.create_tool_calling_agent.html) function.\n",
    "\n",
    "If you are using one of the [old LangChain pre-built agents](https://python.langchain.com/v0.1/docs/modules/agents/agent_types/), you should be able\n",
    "to replace that code with the new [langgraph pre-built agent](https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/) which leverages\n",
    "native tool calling capabilities of chat models and will likely work better out of the box.\n",
    "\n",
    "### Legacy Usage\n",
    "\n",
    "<details open>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc2928de-d7a4-4f87-ab96-59bde9a3829f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eugene/.pyenv/versions/3.11.4/envs/core/lib/python3.11/site-packages/langsmith/client.py:5301: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  prompt = loads(json.dumps(prompt_object.manifest))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'hi! my name is bob what is my age?', 'chat_history': [HumanMessage(content='hi! my name is bob what is my age?', additional_kwargs={}, response_metadata={}), AIMessage(content='Hello Bob! You are 42 years old. If you need any more assistance or information, feel free to ask!', additional_kwargs={}, response_metadata={})], 'output': 'Hello Bob! You are 42 years old. If you need any more assistance or information, feel free to ask!'}\n",
      "\n",
      "{'input': 'do you remember my name?', 'chat_history': [HumanMessage(content='hi! my name is bob what is my age?', additional_kwargs={}, response_metadata={}), AIMessage(content='Hello Bob! You are 42 years old. If you need any more assistance or information, feel free to ask!', additional_kwargs={}, response_metadata={}), HumanMessage(content='do you remember my name?', additional_kwargs={}, response_metadata={}), AIMessage(content='Yes, your name is Bob. How can I assist you further, Bob?', additional_kwargs={}, response_metadata={})], 'output': 'Yes, your name is Bob. How can I assist you further, Bob?'}\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(temperature=0)\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_user_age(name: str) -> str:\n",
    "    \"\"\"Use this tool to find the user's age.\"\"\"\n",
    "    # This is a placeholder for the actual implementation\n",
    "    if \"bob\" in name.lower():\n",
    "        return \"42 years old\"\n",
    "    return \"41 years old\"\n",
    "\n",
    "\n",
    "tools = [get_user_age]\n",
    "\n",
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "\n",
    "# Instantiate memory\n",
    "# highlight-start\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "# highlight-end\n",
    "\n",
    "# Create an agent\n",
    "agent = create_tool_calling_agent(model, tools, prompt)\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    # highlight-next-line\n",
    "    memory=memory,  # Pass the memory to the executor\n",
    ")\n",
    "\n",
    "# Verify that the agent can use tools\n",
    "print(agent_executor.invoke({\"input\": \"hi! my name is bob what is my age?\"}))\n",
    "print()\n",
    "# Verify that the agent has access to conversation history.\n",
    "# The agent should be able to answer that the user's name is bob.\n",
    "print(agent_executor.invoke({\"input\": \"do you remember my name?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4866ae9-e683-44dc-a77b-da1737d3a645",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "### LangGraph\n",
    "\n",
    "This example shows how to add memory to the pre-built react agent in langgraph. This is an agent for which you only need to supply the tool and prompt.\n",
    "\n",
    "After looking through the snippet below, you can reference [this guide](https://langchain-ai.github.io/langgraph/how-tos/create-react-agent-memory/) for more details.\n",
    "\n",
    "<details open>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdb29c9b-bc57-4512-9430-c5d5e3f91e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "hi! I'm bob. What is my age?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  get_user_age (call_YIxwgZJ2IzpwPzcmLzIDdt3w)\n",
      " Call ID: call_YIxwgZJ2IzpwPzcmLzIDdt3w\n",
      "  Args:\n",
      "    name: bob\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: get_user_age\n",
      "\n",
      "42 years old\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Bob, you are 42 years old.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "do you remember my name?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Yes, your name is Bob.\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_user_age(name: str) -> str:\n",
    "    \"\"\"Use this tool to find the user's age.\"\"\"\n",
    "    # This is a placeholder for the actual implementation\n",
    "    if \"bob\" in name.lower():\n",
    "        return \"42 years old\"\n",
    "    return \"41 years old\"\n",
    "\n",
    "\n",
    "# highlight-next-line\n",
    "memory = MemorySaver()\n",
    "model = ChatOpenAI()\n",
    "app = create_react_agent(\n",
    "    model,\n",
    "    tools=[get_user_age],\n",
    "    # highlight-next-line\n",
    "    checkpointer=memory,\n",
    ")\n",
    "\n",
    "# highlight-start\n",
    "# The thread id is a unique key that identifies\n",
    "# this particular conversation.\n",
    "# We'll just generate a random uuid here.\n",
    "thread_id = uuid.uuid4()\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "# highlight-end\n",
    "\n",
    "# Tell the AI that our name is Bob, and ask it to use a tool to confirm\n",
    "# that it's capable of working like an agent.\n",
    "input_message = HumanMessage(content=\"hi! I'm bob. What is my age?\")\n",
    "\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()\n",
    "\n",
    "# Confirm that the chat bot has access to previous conversation\n",
    "# and can respond to the user saying that the user's name is Bob.\n",
    "input_message = HumanMessage(content=\"do you remember my name?\")\n",
    "\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe63e424-1111-4f6a-a9c9-0887eb150ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "hi! do you remember my name?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello! Yes, I remember your name. It's great to see you again. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# Now, let's confirm that if we use a different thread\n",
    "# that the bot will not know what our name is since\n",
    "# it's a new conversation!\n",
    "config = {\"configurable\": {\"thread_id\": \"123456789\"}}\n",
    "\n",
    "input_message = HumanMessage(content=\"hi! do you remember my name?\")\n",
    "\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2717810",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "Explore persistence with LangGraph:\n",
    "\n",
    "* [LangGraph quickstart tutorial](https://langchain-ai.github.io/langgraph/tutorials/introduction/)\n",
    "* [How to add persistence (\"memory\") to your graph](https://langchain-ai.github.io/langgraph/how-tos/persistence/)\n",
    "* [How to manage conversation history](https://langchain-ai.github.io/langgraph/how-tos/memory/manage-conversation-history/)\n",
    "* [How to add summary of the conversation history](https://langchain-ai.github.io/langgraph/how-tos/memory/add-summary-conversation-history/)\n",
    "\n",
    "Add persistence with simple LCEL (favor langgraph for more complex use cases):\n",
    "\n",
    "* [How to add message history](/docs/how_to/message_history/)\n",
    "\n",
    "Working with message history:\n",
    "\n",
    "* [How to trim messages](/docs/how_to/trim_messages)\n",
    "* [How to filter messages](/docs/how_to/filter_messages/)\n",
    "* [How to merge message runs](/docs/how_to/merge_message_runs/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4c48e1-b613-4aab-bc2b-617c811fad1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
