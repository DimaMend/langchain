{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "575befea-4d98-4941-8e55-1581b169a674",
   "metadata": {},
   "source": [
    "---\n",
    "title: Migrating from MultiPromptChain\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14625d35-efca-41cf-b203-be9f4c375700",
   "metadata": {},
   "source": [
    "The [`MultiPromptChain`](https://api.python.langchain.com/en/latest/chains/langchain.chains.router.multi_prompt.MultiPromptChain.html) routed an input query to one of multiple LLMChains-- that is, given an input query, it used a LLM to select from a list of prompts, formatted the query into the prompt, and generated a response.\n",
    "\n",
    "`MultiPromptChain` does not support common [chat model](/docs/concepts/#chat-models) features, such as message roles and [tool calling](/docs/concepts/#functiontool-calling).\n",
    "\n",
    "Some advantages of switching to the LCEL implementation are:\n",
    "\n",
    "- Supports chat prompt templates, including messages with `system` and other roles;\n",
    "- Supports the use of tool calling for the routing step;\n",
    "- Support for runnable methods like streaming and async operations.\n",
    "\n",
    "Now let's look at them side-by-side. Note that for this guide we will `langchain-openai >= 0.1.20`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba0c648-63e9-4f4a-b4ba-cd36fcb21466",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain-core langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05ae3aa-96cc-49f2-8dd0-601c2503b7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707283ee-fbd8-43e0-9796-343fc2534658",
   "metadata": {},
   "source": [
    "import { ColumnContainer, Column } from \"@theme/Columns\";\n",
    "\n",
    "<ColumnContainer>\n",
    "\n",
    "<Column>\n",
    "\n",
    "#### Legacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cca7a1a-3ce5-4c60-9664-cbbff14fc7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.router.multi_prompt import MultiPromptChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "prompt_1_template = \"\"\"\n",
    "You are an expert on animals. Please answer the beloq query:\n",
    "\n",
    "{input}\n",
    "\"\"\"\n",
    "\n",
    "prompt_2_template = \"\"\"\n",
    "You are an expert on vegetables. Please answer the beloq query:\n",
    "\n",
    "{input}\n",
    "\"\"\"\n",
    "\n",
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"animals\",\n",
    "        \"description\": \"prompt for an animal expert\",\n",
    "        \"prompt_template\": prompt_1_template,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"vegetables\",\n",
    "        \"description\": \"prompt for a vegetable expert\",\n",
    "        \"prompt_template\": prompt_2_template,\n",
    "    },\n",
    "]\n",
    "\n",
    "chain = MultiPromptChain.from_prompts(llm, prompt_infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcb635d2-0402-4afb-ab64-044811d5348c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What colors are carrots?',\n",
       " 'text': 'Carrots come in various colors such as orange, red, yellow, purple, and white. The most common color of carrots is orange, but there are also heirloom varieties that come in different colors.'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"What color are carrots?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ff8cd1-7e47-4133-a793-1ccd61a57f04",
   "metadata": {},
   "source": [
    "\n",
    "</Column>\n",
    "\n",
    "<Column>\n",
    "\n",
    "#### LCEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf1edd2b-7592-47f4-ba8d-94a56742a585",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from typing import Literal\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableBranch, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "# Define the prompts we will route to\n",
    "prompt_1 = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an expert on animals.\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "prompt_2 = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an expert on vegetables.\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Construct the chains we will route to. These format the input query\n",
    "# into the respective prompt, run it through a chat model, and cast\n",
    "# the result to a string.\n",
    "chain_1 = prompt_1 | llm | StrOutputParser()\n",
    "chain_2 = prompt_2 | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "# Next: define the chain that selects which branch to route to.\n",
    "# Here we will take advantage of tool-calling features to force\n",
    "# the output to select one of two desired branches.\n",
    "route_system = \"Route the user's query to either the animal or vegetable expert.\"\n",
    "route_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", route_system),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Define schema for output:\n",
    "class RouteQuery(TypedDict):\n",
    "    \"\"\"Route query to destination expert.\"\"\"\n",
    "\n",
    "    destination: Literal[\"animal\", \"vegetable\"]\n",
    "\n",
    "\n",
    "route_chain = (\n",
    "    route_prompt | llm.with_structured_output(RouteQuery) | itemgetter(\"destination\")\n",
    ")\n",
    "\n",
    "\n",
    "# Finally, assemble the multi-prompt chain. This is a sequence of two steps:\n",
    "# 1) Select \"animal\" or \"vegetable\" via the route_chain, and collect the answer\n",
    "# alongside the input query.\n",
    "# 2) Use RunnableBranch to route the input query to chain_1 or chain_2, based on the\n",
    "# selection.\n",
    "chain = {\n",
    "    \"destination\": route_chain,  # \"animal\" or \"vegetable\"\n",
    "    \"input\": RunnablePassthrough(),  # pass through input query\n",
    "} | RunnableBranch(\n",
    "    (lambda x: x[\"destination\"] == \"animal\", chain_1),  # if animal, chain_1\n",
    "    chain_2,  # otherwise, chain_2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61838f81-4e60-445f-9c05-563e3520ab33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Carrots are typically orange in color, but they can also be found in other colors such as purple, red, yellow, and white, depending on the variety.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"what color are carrots\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e46205-9d80-45b8-a3d5-cfbc8ebbe19a",
   "metadata": {},
   "source": [
    "\n",
    "</Column>\n",
    "</ColumnContainer>\n",
    "\n",
    "## Overview:\n",
    "\n",
    "- Under the hood, `MultiPromptChain` routes the query by instructing the LLM to generate JSON-formatted text, and parses out the intended destination. It takes a registry of string prompt templates as input.\n",
    "- The LCEL implementation, implemented above via lower-level primitives, uses tool-calling to route to arbitrary chains. In this example, the chains include chat model templates and chat models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf89de23-377b-4933-839c-d2f2483d09d2",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "See [this tutorial](/docs/tutorials/llm_chain) for more detail on building with prompt templates, LLMs, and output parsers.\n",
    "\n",
    "Check out the [LCEL conceptual docs](/docs/concepts/#langchain-expression-language-lcel) for more background information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
