{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32eee276-7847-45d8-b303-dccc330c8a1a",
   "metadata": {},
   "source": [
    "---\n",
    "title: Migrating from RefineDocumentsChain\n",
    "---\n",
    "\n",
    "[RefineDocumentsChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.combine_documents.refine.RefineDocumentsChain.html) implements a strategy for analyzing long texts. The strategy is as follows:\n",
    "\n",
    "- Split a text into smaller documents;\n",
    "- Apply a process to the first document;\n",
    "- Refine or update the result based on the next document;\n",
    "- Repeat through the sequence of documents until finished.\n",
    "\n",
    "A common process applied in this context is summarization, in which a running summary is modified as we proceed through chunks of a long text. This is particularly useful for texts that are large compared to the context window of a given LLM.\n",
    "\n",
    "An [LCEL](/docs/concepts/#langchain-expression-language-lcel) implementation confers a number of advantages for this problem, including support for token-by-token streaming. Because it is assembled from modular components, it is also simple to extend or modify (e.g., to incorporate [tool calling](/docs/concepts/#functiontool-calling) or other behavior).\n",
    "\n",
    "Below we will go through both `RefineDocumentsChain` and a corresponding LCEL implementation on a simple example for illustrative purposes.\n",
    "\n",
    "Let's first load a chat model:\n",
    "\n",
    "```{=mdx}\n",
    "import ChatModelTabs from \"@theme/ChatModelTabs\";\n",
    "\n",
    "<ChatModelTabs customVarName=\"llm\" />\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34fc8315-4354-4d4e-952a-c0465d93b23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9950d6e1-7ca0-4b46-8622-813b3c30b85d",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Let's go through an example where we summarize a sequence of documents. We first generate some simple documents for illustrative purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abb1abb0-0c5e-4179-8431-c2b2d52bd57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "documents = [\n",
    "    Document(page_content=\"Apples are red\", metadata={\"title\": \"apple_book\"}),\n",
    "    Document(page_content=\"Blueberries are blue\", metadata={\"title\": \"blueberry_book\"}),\n",
    "    Document(page_content=\"Bananas are yelow\", metadata={\"title\": \"banana_book\"}),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd939c9-1717-4afd-a615-4a1d560ca814",
   "metadata": {},
   "source": [
    "### Legacy\n",
    "\n",
    "Below we show an implementation with `RefineDocumentsChain`. We define the prompt templates for the initial summarization and successive refinements, instantiate separate [LLMChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.llm.LLMChain.html) objects for these two purposes, and instantiate `RefineDocumentsChain` with these components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b2dd248-8bf3-40a5-9569-df32558b5d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain, RefineDocumentsChain\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# This controls how each document will be formatted. Specifically,\n",
    "# it will be passed to `format_document` - see that function for more\n",
    "# details.\n",
    "document_prompt = PromptTemplate(\n",
    "    input_variables=[\"page_content\"], template=\"{page_content}\"\n",
    ")\n",
    "document_variable_name = \"context\"\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "# The prompt here should take as an input variable the\n",
    "# `document_variable_name`\n",
    "summarize_prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"human\", \"Write a concise summary of the following: {context}\"),\n",
    "    ]\n",
    ")\n",
    "initial_llm_chain = LLMChain(llm=llm, prompt=summarize_prompt)\n",
    "initial_response_name = \"existing_answer\"\n",
    "# The prompt here should take as an input variable the\n",
    "# `document_variable_name` as well as `initial_response_name`\n",
    "refine_template = \"\"\"\n",
    "Produce a final summary.\n",
    "\n",
    "Existing summary up to this point:\n",
    "{existing_answer}\n",
    "\n",
    "New context:\n",
    "------------\n",
    "{context}\n",
    "------------\n",
    "\n",
    "Given the new context, refine the original summary.\n",
    "\"\"\"\n",
    "refine_prompt = ChatPromptTemplate([(\"human\", refine_template)])\n",
    "refine_llm_chain = LLMChain(llm=llm, prompt=refine_prompt)\n",
    "chain = RefineDocumentsChain(\n",
    "    initial_llm_chain=initial_llm_chain,\n",
    "    refine_llm_chain=refine_llm_chain,\n",
    "    document_prompt=document_prompt,\n",
    "    document_variable_name=document_variable_name,\n",
    "    initial_response_name=initial_response_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ee50d8-5f37-4bcd-9181-5280b54b1b44",
   "metadata": {},
   "source": [
    "We can now invoke our chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8686f56f-992f-4556-a74c-8d3903d0db38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apples are red, blueberries are blue, and bananas are yellow in color.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = chain.invoke(documents)\n",
    "result[\"output_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d5f5d2-b7f3-431c-bb25-fa1b4d353663",
   "metadata": {},
   "source": [
    "The [LangSmith trace](https://smith.langchain.com/public/8ec51479-9420-412f-bb21-cb8c9f59dfde/r) is composed of three LLM calls: one for the initial summary, and two more updates of that summary. The process completes when we update the summary with content from the final document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5caba4-f363-4bcf-8dd4-1d015e27a18d",
   "metadata": {},
   "source": [
    "### LCEL\n",
    "\n",
    "Below we show an LCEL implementation of this process:\n",
    "\n",
    "- We use the same two templates as before.\n",
    "- We generate a simple chain for the initial summary that plucks out the first document, formats it into a prompt and runs inference with our LLM.\n",
    "- We generate a second `_refine_remaining_docs` chain that generates a sequence of simple `prompt | llm | StrOutputParser()` chains, formatting the context from each remaining document into the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "044fee44-bbbf-4e81-97c7-4c1168d5e087",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import (\n",
    "    RunnableLambda,\n",
    "    RunnablePassthrough,\n",
    "    RunnableSequence,\n",
    ")\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "summarize_prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"human\", \"Write a concise summary of the following: {context}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "initial_summary = (\n",
    "    RunnableLambda(lambda docs: docs[0].page_content)  # select first document\n",
    "    | summarize_prompt  # Format content into summarize prompt\n",
    "    | llm  # Generate response\n",
    "    | StrOutputParser()  # Cast to string\n",
    ")\n",
    "\n",
    "refine_template = \"\"\"\n",
    "Produce a final summary.\n",
    "\n",
    "Existing summary up to this point:\n",
    "{existing_answer}\n",
    "\n",
    "New context:\n",
    "------------\n",
    "{context}\n",
    "------------\n",
    "\n",
    "Given the new context, refine the original summary.\n",
    "\"\"\"\n",
    "refine_prompt = ChatPromptTemplate([(\"human\", refine_template)])\n",
    "\n",
    "\n",
    "def _refine_remaining_docs(input_dict: dict) -> RunnableSequence:\n",
    "    \"\"\"Create a sequence of summarizers, each adding context from a document.\"\"\"\n",
    "    steps = []\n",
    "    for document in input_dict[\"documents\"]:\n",
    "        steps.append(\n",
    "            # format text from document[i] into refine prompt\n",
    "            refine_prompt.partial(context=document.page_content)\n",
    "            | llm  # generate response\n",
    "            | StrOutputParser()  # cast to string\n",
    "        )\n",
    "    return RunnableSequence(*steps)\n",
    "\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        # create initial summary\n",
    "        \"existing_answer\": initial_summary,\n",
    "        # pass along remaining documents\n",
    "        \"documents\": RunnableLambda(lambda docs: docs[1:]),\n",
    "    }\n",
    "    # create and run a sequence of summaries on remaining docs\n",
    "    | RunnableLambda(_refine_remaining_docs)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca52e64b-5828-4220-96ac-4dc01668b5ec",
   "metadata": {},
   "source": [
    "We can invoke the chain as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1aea299-c62e-4e36-a3e5-9ed866169f39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apples are typically red in color, blueberries are blue, and bananas are yellow.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb390e43-58db-43af-9118-aed6f08ce351",
   "metadata": {},
   "source": [
    "In the [LangSmith trace](https://smith.langchain.com/public/6b038c7a-c56b-4ae2-9d82-686020146c43/r) we again recover three LLM calls, performing the same functions as before.\n",
    "\n",
    "Note that the LCEL implementation allows us to stream output tokens, if desired:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20be31b7-7981-408e-9962-b525bfc89623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | App | les |  are |  a |  red |  fruit | , |  blue | berries |  are |  blue | , |  and |  bananas |  are |  yellow | . |  | "
     ]
    }
   ],
   "source": [
    "for chunk in chain.stream(documents):\n",
    "    print(chunk, end=\" | \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad2a8f1-e453-4f0b-8911-8c8ddf607f64",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "Check out the [LCEL conceptual docs](/docs/concepts/#langchain-expression-language-lcel) for more background information.\n",
    "\n",
    "See [this tutorial](/docs/tutorials/summarization/) for more LLM-based summarization strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52a8a4b-4dbe-4a82-9267-e9a7c7fa188d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
