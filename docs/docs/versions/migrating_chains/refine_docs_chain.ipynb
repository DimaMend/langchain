{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32eee276-7847-45d8-b303-dccc330c8a1a",
   "metadata": {},
   "source": [
    "---\n",
    "title: Migrating from RefineDocumentsChain\n",
    "---\n",
    "\n",
    "[RefineDocumentsChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.combine_documents.refine.RefineDocumentsChain.html) implements a strategy for analyzing long texts. The strategy is as follows:\n",
    "\n",
    "- Split a text into smaller documents;\n",
    "- Apply a process to the first document;\n",
    "- Refine or update the result based on the next document;\n",
    "- Repeat through the sequence of documents until finished.\n",
    "\n",
    "A common process applied in this context is summarization, in which a running summary is modified as we proceed through chunks of a long text. This is particularly useful for texts that are large compared to the context window of a given LLM.\n",
    "\n",
    "A [LangGraph](https://langchain-ai.github.io/langgraph/) implementation confers a number of advantages for this problem, including support for token-by-token streaming. Because it is assembled from modular components, it is also simple to extend or modify (e.g., to incorporate [tool calling](/docs/concepts/#functiontool-calling) or other behavior).\n",
    "\n",
    "Below we will go through both `RefineDocumentsChain` and a corresponding LangGraph implementation on a simple example for illustrative purposes.\n",
    "\n",
    "Let's first load a chat model:\n",
    "\n",
    "```{=mdx}\n",
    "import ChatModelTabs from \"@theme/ChatModelTabs\";\n",
    "\n",
    "<ChatModelTabs customVarName=\"llm\" />\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34fc8315-4354-4d4e-952a-c0465d93b23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9950d6e1-7ca0-4b46-8622-813b3c30b85d",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Let's go through an example where we summarize a sequence of documents. We first generate some simple documents for illustrative purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abb1abb0-0c5e-4179-8431-c2b2d52bd57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "documents = [\n",
    "    Document(page_content=\"Apples are red\", metadata={\"title\": \"apple_book\"}),\n",
    "    Document(page_content=\"Blueberries are blue\", metadata={\"title\": \"blueberry_book\"}),\n",
    "    Document(page_content=\"Bananas are yelow\", metadata={\"title\": \"banana_book\"}),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd939c9-1717-4afd-a615-4a1d560ca814",
   "metadata": {},
   "source": [
    "### Legacy\n",
    "\n",
    "<details open>\n",
    "\n",
    "Below we show an implementation with `RefineDocumentsChain`. We define the prompt templates for the initial summarization and successive refinements, instantiate separate [LLMChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.llm.LLMChain.html) objects for these two purposes, and instantiate `RefineDocumentsChain` with these components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b2dd248-8bf3-40a5-9569-df32558b5d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain, RefineDocumentsChain\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# This controls how each document will be formatted. Specifically,\n",
    "# it will be passed to `format_document` - see that function for more\n",
    "# details.\n",
    "document_prompt = PromptTemplate(\n",
    "    input_variables=[\"page_content\"], template=\"{page_content}\"\n",
    ")\n",
    "document_variable_name = \"context\"\n",
    "# The prompt here should take as an input variable the\n",
    "# `document_variable_name`\n",
    "summarize_prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"human\", \"Write a concise summary of the following: {context}\"),\n",
    "    ]\n",
    ")\n",
    "initial_llm_chain = LLMChain(llm=llm, prompt=summarize_prompt)\n",
    "initial_response_name = \"existing_answer\"\n",
    "# The prompt here should take as an input variable the\n",
    "# `document_variable_name` as well as `initial_response_name`\n",
    "refine_template = \"\"\"\n",
    "Produce a final summary.\n",
    "\n",
    "Existing summary up to this point:\n",
    "{existing_answer}\n",
    "\n",
    "New context:\n",
    "------------\n",
    "{context}\n",
    "------------\n",
    "\n",
    "Given the new context, refine the original summary.\n",
    "\"\"\"\n",
    "refine_prompt = ChatPromptTemplate([(\"human\", refine_template)])\n",
    "refine_llm_chain = LLMChain(llm=llm, prompt=refine_prompt)\n",
    "chain = RefineDocumentsChain(\n",
    "    initial_llm_chain=initial_llm_chain,\n",
    "    refine_llm_chain=refine_llm_chain,\n",
    "    document_prompt=document_prompt,\n",
    "    document_variable_name=document_variable_name,\n",
    "    initial_response_name=initial_response_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ee50d8-5f37-4bcd-9181-5280b54b1b44",
   "metadata": {},
   "source": [
    "We can now invoke our chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8686f56f-992f-4556-a74c-8d3903d0db38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apples are typically red in color, blueberries are blue, and bananas are yellow.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = chain.invoke(documents)\n",
    "result[\"output_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d5f5d2-b7f3-431c-bb25-fa1b4d353663",
   "metadata": {},
   "source": [
    "The [LangSmith trace](https://smith.langchain.com/public/8ec51479-9420-412f-bb21-cb8c9f59dfde/r) is composed of three LLM calls: one for the initial summary, and two more updates of that summary. The process completes when we update the summary with content from the final document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5caba4-f363-4bcf-8dd4-1d015e27a18d",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "### LangGraph\n",
    "\n",
    "<details open>\n",
    "\n",
    "Below we show a LangGraph implementation of this process:\n",
    "\n",
    "- We use the same two templates as before.\n",
    "- We generate a simple chain for the initial summary that plucks out the first document, formats it into a prompt and runs inference with our LLM.\n",
    "- We generate a second `refine_summary_chain` operates on each successive document, refining the initial summary.\n",
    "\n",
    "We will need to install `langgraph`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda06266-f4fe-43cf-9044-0ce5ee76c793",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -qU langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3477bef6-97cc-492f-87fe-cf5336edd581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, List, TypedDict\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.constants import Send\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Initial summary\n",
    "summarize_prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"human\", \"Write a concise summary of the following: {context}\"),\n",
    "    ]\n",
    ")\n",
    "initial_summary_chain = summarize_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Refining the summary with new docs\n",
    "refine_template = \"\"\"\n",
    "Produce a final summary.\n",
    "\n",
    "Existing summary up to this point:\n",
    "{existing_answer}\n",
    "\n",
    "New context:\n",
    "------------\n",
    "{context}\n",
    "------------\n",
    "\n",
    "Given the new context, refine the original summary.\n",
    "\"\"\"\n",
    "refine_prompt = ChatPromptTemplate([(\"human\", refine_template)])\n",
    "\n",
    "refine_summary_chain = refine_prompt | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    contents: List[str]\n",
    "    summary: str\n",
    "\n",
    "\n",
    "# Here we generate an answer with score, given a document\n",
    "async def generate_summary(state: State, config: RunnableConfig):\n",
    "    summary = await initial_summary_chain.ainvoke(\n",
    "        state[\"contents\"][0],\n",
    "        config,\n",
    "    )\n",
    "\n",
    "    for content in state[\"contents\"][1:]:\n",
    "        summary = await refine_summary_chain.ainvoke(\n",
    "            {\"existing_answer\": summary, \"context\": content},\n",
    "            config,\n",
    "        )\n",
    "\n",
    "    return {\"summary\": summary}\n",
    "\n",
    "\n",
    "graph = StateGraph(State)\n",
    "graph.add_node(\"generate_summary\", generate_summary)\n",
    "graph.add_edge(START, \"generate_summary\")\n",
    "graph.add_edge(\"generate_summary\", END)\n",
    "app = graph.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9487118a-25ea-4ecf-982b-10c72bec3325",
   "metadata": {},
   "source": [
    "We can invoke the chain as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9572e4bd-5e7d-4884-8283-d516396d7c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apples are typically red in color, blueberries are blue, and bananas are yellow.\n"
     ]
    }
   ],
   "source": [
    "result = await app.ainvoke({\"contents\": [doc.page_content for doc in documents]})\n",
    "\n",
    "print(result[\"summary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb390e43-58db-43af-9118-aed6f08ce351",
   "metadata": {},
   "source": [
    "In the [LangSmith trace](https://smith.langchain.com/public/6b038c7a-c56b-4ae2-9d82-686020146c43/r) we again recover three LLM calls, performing the same functions as before.\n",
    "\n",
    "Note that we can stream tokens from the application, including from intermediate steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "105d71ee-0eb8-40bf-aa82-c94121dba2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ap|ples| are| characterized| by| their| red| color|.|\n",
      "\n",
      "\n",
      "Ap|ples| are| characterized| by| their| red| color|,| while| blueberries| are| known| for| their| blue| hue|.|\n",
      "\n",
      "\n",
      "Ap|ples| are| characterized| by| their| red| color|,| blueberries| are| known| for| their| blue| hue|,| and| bananas| are| recognized| for| their| yellow| color|.|\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "async for event in app.astream_events(\n",
    "    {\"contents\": [doc.page_content for doc in documents]}, version=\"v2\"\n",
    "):\n",
    "    kind = event[\"event\"]\n",
    "    if kind == \"on_chat_model_stream\":\n",
    "        content = event[\"data\"][\"chunk\"].content\n",
    "        if content:\n",
    "            print(content, end=\"|\")\n",
    "    elif kind == \"on_chat_model_end\":\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad2a8f1-e453-4f0b-8911-8c8ddf607f64",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "## Next steps\n",
    "\n",
    "See [this tutorial](/docs/tutorials/summarization/) for more LLM-based summarization strategies.\n",
    "\n",
    "Check out the [LangGraph documentation](https://langchain-ai.github.io/langgraph/) for detail on building with LangGraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52a8a4b-4dbe-4a82-9267-e9a7c7fa188d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
