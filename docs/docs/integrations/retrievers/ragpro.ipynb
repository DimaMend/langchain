{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAGProRetriever\n",
    "\n",
    "## Overview\n",
    "This retriever allows you to filter your search by domain and provides higher quality results than just chunks of context; meaning that the need to perform Corrective RAG or worry about low-quality data is gone.\n",
    "\n",
    "Instead, this retriever uses a search engine to find relevant documents with an LLM to provide your RAG app with the highest quality data possible.\n",
    "\n",
    "We are also the first retriever to support url (domain) filtering which means you no longer need to scrape and index specific sites yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-community in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (0.3.2)\n",
      "Requirement already satisfied: langchain in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (0.3.3)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from langchain-community) (0.1.134)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from langchain-community) (2.0.35)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from langchain-community) (2.5.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from langchain-community) (3.10.10)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.10 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from langchain-community) (0.3.10)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from langchain-community) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from langchain-community) (8.5.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from langchain) (0.3.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from langchain) (2.9.2)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.15.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.22.0)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.10->langchain-community) (24.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.10->langchain-community) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.10->langchain-community) (4.12.2)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (3.10.7)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (2024.8.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (2.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (3.4.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.6)\n",
      "Requirement already satisfied: sniffio in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: anyio in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (4.6.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.10->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.2.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade langchain-community langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get your api key [here](https://docs.rag.pro/account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import RAGProRetriever\n",
    "\n",
    "retriever = RAGProRetriever(\n",
    "    api_key=\"RAG-PRO-API-KEY\",\n",
    "    # Choose between small, medium, or large model sizes depending on your use case\n",
    "    model=\"small\",\n",
    "    include_sources=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[43mretriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYour query here\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(docs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mpage_content[:\u001b[38;5;241m400\u001b[39m])\n",
      "File \u001b[0;32m~/langchain-1/venv/lib/python3.10/site-packages/langchain_core/retrievers.py:247\u001b[0m, in \u001b[0;36mBaseRetriever.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m _kwargs \u001b[38;5;241m=\u001b[39m kwargs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expects_other_args \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_arg_supported:\n\u001b[0;32m--> 247\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_kwargs\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    251\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_relevant_documents(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs)\n",
      "File \u001b[0;32m~/langchain-1/libs/community/langchain_community/retrievers/rag_pro.py:42\u001b[0m, in \u001b[0;36mRAGProRetriever._get_relevant_documents\u001b[0;34m(self, query, run_manager, urls)\u001b[0m\n\u001b[1;32m     31\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m: query,\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msources\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minclude_sources)\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m     35\u001b[0m }\n\u001b[1;32m     37\u001b[0m data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqueryStringParameters\u001b[39m\u001b[38;5;124m\"\u001b[39m: params,\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiValueQueryStringParameters\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murls\u001b[39m\u001b[38;5;124m\"\u001b[39m: urls} \u001b[38;5;28;01mif\u001b[39;00m urls \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m     40\u001b[0m }\n\u001b[0;32m---> 42\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_url\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/retrieve\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m     45\u001b[0m results \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m~/langchain-1/venv/lib/python3.10/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/langchain-1/venv/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/langchain-1/venv/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/langchain-1/venv/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/langchain-1/venv/lib/python3.10/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/langchain-1/venv/lib/python3.10/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/langchain-1/venv/lib/python3.10/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/langchain-1/venv/lib/python3.10/site-packages/urllib3/connection.py:507\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 507\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/http/client.py:1374\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1373\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1374\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1375\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1376\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1271\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1272\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1273\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1130\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "docs = retriever.invoke(\"Your query here\")\n",
    "\n",
    "print(docs[0].page_content[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 1:\n",
      "To create a ReAct agent graph using OpenAI as the Large Language Model (LLM) in LangGraph, you can follow these steps:\n",
      "\n",
      "## Install Required Packages\n",
      "Ensure you have the necessary packages installed. You can do this using the following command:\n",
      "\n",
      "```python\n",
      "%pip install -U langgraph langchain-openai\n",
      "```\n",
      "\n",
      "## Set Up API Keys\n",
      "Set your OpenAI API key as an environment variable:\n",
      "\n",
      "```python\n",
      "import getpass\n",
      "import os\n",
      "\n",
      "def _set_env(var: str):\n",
      "    if not os.environ.get(var):\n",
      "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
      "\n",
      "_set_env(\"OPENAI_API_KEY\")\n",
      "```\n",
      "\n",
      "## Initialize the Model\n",
      "Initialize the OpenAI model you want to use:\n",
      "\n",
      "```python\n",
      "from langchain_openai import ChatOpenAI\n",
      "\n",
      "model = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
      "```\n",
      "\n",
      "## Define Tools (Optional)\n",
      "If you want to use tools with your agent, define them using the `@tool` decorator:\n",
      "\n",
      "```python\n",
      "from typing import Literal\n",
      "from langchain_core.tools import tool\n",
      "\n",
      "@tool\n",
      "def get_weather(city: Literal[\"nyc\", \"sf\"]):\n",
      "    \"\"\"Use this to get weather information.\"\"\"\n",
      "    if city == \"nyc\":\n",
      "        return \"It might be cloudy in nyc\"\n",
      "    elif city == \"sf\":\n",
      "        return \"It's always sunny in sf\"\n",
      "    else:\n",
      "        raise AssertionError(\"Unknown city\")\n",
      "\n",
      "tools = [get_weather]\n",
      "```\n",
      "\n",
      "## Create the ReAct Agent Graph\n",
      "You can either create the graph from scratch or use the prebuilt `create_react_agent` function. Here is how to do it using the prebuilt function:\n",
      "\n",
      "```python\n",
      "from langgraph.prebuilt import create_react_agent\n",
      "\n",
      "graph = create_react_agent(model, tools=tools)\n",
      "```\n",
      "\n",
      "### Creating from Scratch\n",
      "If you prefer to create the graph from scratch, here is an example:\n",
      "\n",
      "```python\n",
      "from langgraph.graph import StateGraph, END\n",
      "from langchain_core.messages import BaseMessage\n",
      "from langgraph.graph.message import add_messages\n",
      "from typing import Annotated, Sequence, TypedDict\n",
      "\n",
      "class AgentState(TypedDict):\n",
      "    \"\"\"The state of the agent.\"\"\"\n",
      "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
      "\n",
      "workflow = StateGraph(AgentState)\n",
      "\n",
      "# Define the nodes\n",
      "workflow.add_node(\"agent\", model)\n",
      "workflow.add_node(\"tools\", ToolNode(tools))\n",
      "\n",
      "# Set the entry point\n",
      "workflow.set_entry_point(\"agent\")\n",
      "\n",
      "# Define conditional edges\n",
      "def should_continue(state):\n",
      "    # Implement logic to decide whether to continue or end\n",
      "    # For example:\n",
      "    last_message = state[\"messages\"][-1]\n",
      "    if \"tool\" in last_message.content:\n",
      "        return \"continue\"\n",
      "    else:\n",
      "        return \"end\"\n",
      "\n",
      "workflow.add_conditional_edges(\n",
      "    \"agent\",\n",
      "    should_continue,\n",
      "    {\n",
      "        \"continue\": \"tools\",\n",
      "        \"end\": END,\n",
      "    }\n",
      ")\n",
      "\n",
      "# Add edge from tools to agent\n",
      "workflow.add_edge(\"tools\", \"agent\")\n",
      "\n",
      "# Compile the graph\n",
      "graph = workflow.compile()\n",
      "```\n",
      "\n",
      "## Visualize and Use the Graph\n",
      "You can visualize the graph and use it to process inputs:\n",
      "\n",
      "```python\n",
      "from IPython.display import Image, display\n",
      "\n",
      "display(Image(graph.get_graph().draw_mermaid_png()))\n",
      "\n",
      "def print_stream(stream):\n",
      "    for s in stream:\n",
      "        message = s[\"messages\"][-1]\n",
      "        if isinstance(message, tuple):\n",
      "            print(message)\n",
      "        else:\n",
      "            message.pretty_print()\n",
      "\n",
      "inputs = {\"messages\": [(\"user\", \"what is the weather in sf\")]}\n",
      "print_stream(graph.stream(inputs, stream_mode=\"values\"))\n",
      "```\n",
      "\n",
      "This setup allows you to create a ReAct agent using OpenAI as the LLM and integrate tools if necessary, all within the LangGraph framework[2][4][5].\n",
      "Source: ['https://langchain-ai.github.io/langgraph/how-tos/react-agent-from-scratch/', 'https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/', 'https://langchain-ai.github.io/langgraph/how-tos/create-react-agent-memory/', 'https://langchain-ai.github.io/langgraph/how-tos/streaming-tokens/', 'https://python.langchain.com/docs/how_to/migrate_agent/']\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Define your query and source filter\n",
    "query = \"How can I create a react agent graph with openai as the llm?\"\n",
    "source_filter = [\"https://python.langchain.com/api_reference/\", \"https://langchain-ai.github.io/langgraph/#example\"]\n",
    "\n",
    "# Retrieve documents\n",
    "docs = retriever.get_relevant_documents(query, urls=source_filter)\n",
    "\n",
    "# Print the full content of each document\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(doc.page_content)  # This will print the entire content\n",
    "    print(f\"Source: {doc.metadata.get('source', 'Not available')}\")\n",
    "    print(\"-\" * 50)  # Separator between documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use within a chain\n",
    "Like other retrievers, `RAGProRetriever` can be integrated into LLM applications via chains.\n",
    "\n",
    "We will need a LLM or chat model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-openai\n",
      "  Using cached langchain_openai-0.2.2-py3-none-any.whl (49 kB)\n",
      "Collecting openai<2.0.0,>=1.40.0\n",
      "  Using cached openai-1.51.2-py3-none-any.whl (383 kB)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.9 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from langchain-openai) (0.3.10)\n",
      "Collecting tiktoken<1,>=0.7\n",
      "  Using cached tiktoken-0.8.0-cp310-cp310-macosx_11_0_arm64.whl (982 kB)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.9->langchain-openai) (24.1)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.9->langchain-openai) (6.0.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.9->langchain-openai) (2.9.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.9->langchain-openai) (4.12.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.9->langchain-openai) (1.33)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.9->langchain-openai) (8.5.0)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.9->langchain-openai) (0.1.134)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (0.27.2)\n",
      "Collecting tqdm>4\n",
      "  Using cached tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Collecting distro<2,>=1.7.0\n",
      "  Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (4.6.2)\n",
      "Requirement already satisfied: sniffio in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (1.3.1)\n",
      "Collecting jiter<1,>=0.4.0\n",
      "  Using cached jiter-0.6.1-cp310-cp310-macosx_11_0_arm64.whl (301 kB)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
      "Collecting regex>=2022.1.18\n",
      "  Using cached regex-2024.9.11-cp310-cp310-macosx_11_0_arm64.whl (284 kB)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain-openai) (3.10)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain-openai) (1.2.2)\n",
      "Requirement already satisfied: certifi in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain-openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain-openai) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain-openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.9->langchain-openai) (3.0.0)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.9->langchain-openai) (1.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.9->langchain-openai) (3.10.7)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.9->langchain-openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.9->langchain-openai) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.2.3)\n",
      "Installing collected packages: tqdm, regex, jiter, distro, tiktoken, openai, langchain-openai\n",
      "Successfully installed distro-1.9.0 jiter-0.6.1 langchain-openai-0.2.2 openai-1.51.2 regex-2024.9.11 tiktoken-0.8.0 tqdm-4.66.5\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Chain with Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To create a ReAct agent graph using OpenAI as the Large Language Model (LLM), follow these steps:\n",
      "\n",
      "1. **Setup and Dependencies**: Install the necessary packages and set up your OpenAI API key.\n",
      "   ```bash\n",
      "   %pip install -U langgraph langchain-openai\n",
      "   ```\n",
      "\n",
      "   ```python\n",
      "   import getpass\n",
      "   import os\n",
      "\n",
      "   def _set_env(var: str):\n",
      "       if not os.environ.get(var):\n",
      "           os.environ[var] = getpass.getpass(f\"{var}: \")\n",
      "\n",
      "   _set_env(\"OPENAI_API_KEY\")\n",
      "   ```\n",
      "\n",
      "2. **Define the Model and Tools**: Load the OpenAI chat model and define any tools you want to use.\n",
      "   ```python\n",
      "   from langchain_openai import ChatOpenAI\n",
      "   from langchain_core.tools import tool\n",
      "\n",
      "   model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
      "\n",
      "   @tool\n",
      "   def get_weather(city: str):\n",
      "       \"\"\"Use this to get weather information.\"\"\"\n",
      "       if city == \"nyc\":\n",
      "           return \"It might be cloudy in nyc\"\n",
      "       elif city == \"sf\":\n",
      "           return \"It's always sunny in sf\"\n",
      "       else:\n",
      "           raise AssertionError(\"Unknown city\")\n",
      "\n",
      "   tools = [get_weather]\n",
      "   ```\n",
      "\n",
      "3. **Define the Graph State**: Create the state of your agent, which includes a list of messages.\n",
      "   ```python\n",
      "   from typing import Annotated, Sequence, TypedDict\n",
      "   from langchain_core.messages import BaseMessage\n",
      "   from langgraph.graph.message import add_messages\n",
      "\n",
      "   class AgentState(TypedDict):\n",
      "       \"\"\"The state of the agent.\"\"\"\n",
      "       messages: Annotated[Sequence[BaseMessage], add_messages]\n",
      "   ```\n",
      "\n",
      "4. **Define Nodes and Edges**: Create nodes for the agent and tools, and set up the edges to control the flow of the graph.\n",
      "   ```python\n",
      "   from langgraph.graph import StateGraph, END\n",
      "   from langgraph.prebuilt import ToolNode\n",
      "\n",
      "   # Define a new graph\n",
      "   workflow = StateGraph(AgentState)\n",
      "\n",
      "   # Define the tool node\n",
      "   tool_node = ToolNode(tools)\n",
      "\n",
      "   # Define the nodes\n",
      "   workflow.add_node(\"agent\", model)\n",
      "   workflow.add_node(\"tools\", tool_node)\n",
      "\n",
      "   # Set the entry point as `agent`\n",
      "   workflow.set_entry_point(\"agent\")\n",
      "\n",
      "   # Add conditional edges\n",
      "   def should_continue(state):\n",
      "       return \"continue\" if \"tool_call\" in state[\"messages\"][-1].content else \"end\"\n",
      "\n",
      "   workflow.add_conditional_edges(\n",
      "       \"agent\",\n",
      "       should_continue,\n",
      "       {\"continue\": \"tools\", \"end\": END}\n",
      "   )\n",
      "\n",
      "   # Add a normal edge from `tools` to `agent`\n",
      "   workflow.add_edge(\"tools\", \"agent\")\n",
      "\n",
      "   # Compile the graph\n",
      "   graph = workflow.compile()\n",
      "   ```\n",
      "\n",
      "5. **Use the ReAct Agent**: Process inputs and stream the results.\n",
      "   ```python\n",
      "   def print_stream(stream):\n",
      "       for s in stream:\n",
      "           message = s[\"messages\"][-1]\n",
      "           if isinstance(message, tuple):\n",
      "               print(message)\n",
      "           else:\n",
      "               message.pretty_print()\n",
      "\n",
      "   inputs = {\"messages\": [(\"user\", \"what is the weather in sf\")]}\n",
      "   print_stream(graph.stream(inputs, stream_mode=\"values\"))\n",
      "   ```\n",
      "\n",
      "This setup allows you to create a ReAct agent graph using OpenAI as the LLM.\n",
      "\n",
      "Sources:\n",
      "https://langchain-ai.github.io/langgraph/how-tos/create-react-agent-hitl/\n",
      "https://python.langchain.com/docs/how_to/migrate_agent/\n",
      "https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/\n",
      "https://langchain-ai.github.io/langgraph/how-tos/streaming-tokens/\n",
      "https://langchain-ai.github.io/langgraph/how-tos/react-agent-from-scratch/\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Answer the question based only on the context provided.\n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "def format_sources(docs):\n",
    "    sources = []\n",
    "    for doc in docs:\n",
    "        source = doc.metadata.get('source', 'Unknown')\n",
    "        if isinstance(source, list):\n",
    "            sources.extend(source)\n",
    "        else:\n",
    "            sources.append(source)\n",
    "    return \"\\n\".join(set(sources))\n",
    "\n",
    "def retrieve_with_sources(inputs):\n",
    "    query = inputs[\"question\"]\n",
    "    urls = inputs.get(\"urls\", None)\n",
    "    return retriever.get_relevant_documents(query, urls=urls)\n",
    "\n",
    "def process_llm_response(inputs):\n",
    "    llm_response = inputs[\"llm_response\"]\n",
    "    sources = inputs[\"sources\"]\n",
    "    return f\"{llm_response}\\n\\nSources:\\n{sources}\"\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough()\n",
    "    | {\n",
    "        \"context\": RunnableLambda(retrieve_with_sources) | RunnableLambda(format_docs),\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "        \"sources\": RunnableLambda(retrieve_with_sources) | RunnableLambda(format_sources)\n",
    "    }\n",
    "    | {\n",
    "        \"llm_response\": (lambda x: {\"context\": x[\"context\"], \"question\": x[\"question\"]}) | prompt | llm | StrOutputParser(),\n",
    "        \"sources\": lambda x: x[\"sources\"]\n",
    "    }\n",
    "    | RunnableLambda(process_llm_response)\n",
    ")\n",
    "\n",
    "# Example usage of the chain with filtered sources\n",
    "result = chain.invoke({\n",
    "    \"question\": \"How can I create a react agent graph with openai as the llm?\",\n",
    "    \"urls\": [\"https://python.langchain.com/api_reference/\", \"https://langchain-ai.github.io/langgraph/#example\"]\n",
    "})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agentic RAG with RAG Pro Retriever\n",
    "\n",
    "Taken from this [Notebook](https://github.com/langchain-ai/langchain/blob/master/cookbook/langgraph_agentic_rag.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langgraph in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (0.2.35)\n",
      "Collecting langchainhub\n",
      "  Downloading langchainhub-0.1.21-py3-none-any.whl (5.2 kB)\n",
      "Requirement already satisfied: langchain-core<0.4,>=0.2.39 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from langgraph) (0.3.10)\n",
      "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.0 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from langgraph) (2.0.1)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from langchainhub) (24.1)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from langchainhub) (2.32.3)\n",
      "Collecting types-requests<3.0.0.0,>=2.31.0.2\n",
      "  Downloading types_requests-2.32.0.20240914-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from langchain-core<0.4,>=0.2.39->langgraph) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from langchain-core<0.4,>=0.2.39->langgraph) (1.33)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from langchain-core<0.4,>=0.2.39->langgraph) (2.9.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from langchain-core<0.4,>=0.2.39->langgraph) (4.12.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from langchain-core<0.4,>=0.2.39->langgraph) (0.1.134)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from langchain-core<0.4,>=0.2.39->langgraph) (8.5.0)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.1.0 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from langgraph-checkpoint<3.0.0,>=2.0.0->langgraph) (1.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from requests<3,>=2->langchainhub) (3.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from requests<3,>=2->langchainhub) (2024.8.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from requests<3,>=2->langchainhub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from requests<3,>=2->langchainhub) (2.2.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.2.39->langgraph) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.2.39->langgraph) (3.10.7)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.2.39->langgraph) (1.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.2.39->langgraph) (0.27.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.2.39->langgraph) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.2.39->langgraph) (2.23.4)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.2.39->langgraph) (1.0.6)\n",
      "Requirement already satisfied: sniffio in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.2.39->langgraph) (1.3.1)\n",
      "Requirement already satisfied: anyio in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.2.39->langgraph) (4.6.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.2.39->langgraph) (0.14.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/seansullivan/langchain-1/venv/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.2.39->langgraph) (1.2.2)\n",
      "Installing collected packages: types-requests, langchainhub\n",
      "Successfully installed langchainhub-0.1.21 types-requests-2.32.0.20240914\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langgraph langchainhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import (\n",
    "    Annotated,\n",
    "    Sequence,\n",
    "    TypedDict,\n",
    ")\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"The state of the agent.\"\"\"\n",
    "\n",
    "    # add_messages is a reducer\n",
    "    # See https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "@tool\n",
    "def search_langchain(query: str, urls: Optional[List[str]] = None):\n",
    "    \"\"\"\n",
    "    Agent with access to up-to-date information from Langchain and Langgraph's python examples, documentation, and guides.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query.\n",
    "        urls (Optional[List[str]]): List of URLs to filter the search results.\n",
    "                                    If not provided, the search will not be filtered by URL.\n",
    "    \"\"\"\n",
    "    return retriever.get_relevant_documents(query, urls=['https://python.langchain.com/docs/', 'https://langchain-ai.github.io/'])\n",
    "\n",
    "\n",
    "tools = [search_langchain]\n",
    "\n",
    "model = model.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "graph = create_react_agent(model, tools=tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stream(stream):\n",
    "    for s in stream:\n",
    "        message = s[\"messages\"][-1]\n",
    "        if isinstance(message, tuple):\n",
    "            print(message)\n",
    "        else:\n",
    "            message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Create a Math expert react agent graph with openai as the llm that breaks down user's math queries into smaller steps and uses python/calculator tools to accomplish them\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  search_langchain (call_uUDgcUy6KPSvCpPj4Lw8vlfw)\n",
      " Call ID: call_uUDgcUy6KPSvCpPj4Lw8vlfw\n",
      "  Args:\n",
      "    query: Math expert react agent graph with OpenAI LLM\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: search_langchain\n",
      "\n",
      "[Document(metadata={'source': ['https://python.langchain.com/docs/additional_resources/arxiv_references/', 'https://python.langchain.com/docs/integrations/llms/google_vertex_ai_palm/', 'https://python.langchain.com/docs/integrations/document_transformers/markdownify/']}, page_content='To create a math expert React agent graph using OpenAI\\'s LLM, you can leverage the concepts and tools discussed in the LangChain documentation and the ReAct paper. Here’s a step-by-step guide to integrate this:\\n\\n1. **Understand ReAct**: The ReAct approach involves generating both reasoning traces and task-specific actions in an interleaved manner. This allows the model to induce, track, and update action plans while handling exceptions, and it can interface with external sources like knowledge bases or environments to gather additional information[1].\\n\\n2. **Integrate with OpenAI LLM**: You can use OpenAI\\'s GPT-4 model, which is known for its strong language capabilities, to serve as the core controller in your agent. This involves using the GPT-4 model to conduct task planning, select appropriate AI models, execute each subtask, and summarize the response based on execution results[1].\\n\\n3. **Implementing Math Expertise**:\\n   - **Reasoning Traces**: For math problems, you can generate reasoning traces that include step-by-step solutions. This helps in making the model more interpretable and trustworthy.\\n   - **External Tools**: You might need to integrate external tools or APIs for specific mathematical operations, such as calling a calculator API for arithmetic tasks. This integration can be done using tool-augmented language models (TALM) or toolformers, which fine-tune LLMs to learn using external tool APIs[3].\\n\\n4. **Example Code**:\\n   - **Setting Up the Environment**: Ensure you have the necessary packages installed, such as `langchain-google-vertexai` for integrating with Google Cloud models, and `langchain` for general LLM integration.\\n   - **Using GPT-4**: You can invoke GPT-4 using the `langchain-google-vertexai` package to generate responses to mathematical queries.\\n   - **Example Code Snippet**:\\n     ```python\\n     from langchain_google_vertexai import VertexAI\\n\\n     # Initialize the GPT-4 model\\n     model = VertexAI(model_name=\"gpt-4\")\\n\\n     # Define a math problem\\n     math_problem = \"Solve for x in the equation 2x + 5 = 11\"\\n\\n     # Generate the solution\\n     solution = model.invoke(math_problem)\\n\\n     # Print the solution\\n     print(solution)\\n     ```\\n\\n5. **Enhancing with ReAct**:\\n   - **Generate Reasoning Traces**: Use ReAct to generate reasoning traces for the math problem. This involves creating a sequence of steps that lead to the solution.\\n   - **Integrate with External Tools**: If necessary, integrate with external tools like a calculator API to perform specific arithmetic operations.\\n\\nBy following these steps, you can create a math expert React agent graph that leverages OpenAI\\'s LLM capabilities and integrates with external tools as needed.')]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "To create a Math expert React agent graph using OpenAI's LLM that breaks down user math queries into smaller steps and utilizes Python/calculator tools, follow these steps:\n",
      "\n",
      "### Step-by-Step Guide\n",
      "\n",
      "1. **Understand ReAct Approach**: \n",
      "   - The ReAct framework allows the model to generate reasoning traces and perform tasks in an interleaved manner. This helps in tracking action plans and handling exceptions while interacting with external resources.\n",
      "\n",
      "2. **Integrate with OpenAI LLM**:\n",
      "   - Use OpenAI's GPT-4 model as the core controller. This will involve using the model for task planning, selecting suitable actions, executing subtasks, and summarizing results.\n",
      "\n",
      "3. **Implementing Math Expertise**:\n",
      "   - **Reasoning Traces**: Create step-by-step reasoning for math problems to enhance interpretability and trust.\n",
      "   - **External Tools**: Integrate APIs for specific calculations. For example, using a calculator API for arithmetic tasks.\n",
      "\n",
      "4. **Example Code**:\n",
      "   - **Setting Up the Environment**: Install necessary packages like `langchain` and any needed for API integration.\n",
      "   - **Using GPT-4**: Here’s a simple example code snippet to invoke the GPT-4 model:\n",
      "\n",
      "     ```python\n",
      "     from langchain_google_vertexai import VertexAI\n",
      "\n",
      "     # Initialize the GPT-4 model\n",
      "     model = VertexAI(model_name=\"gpt-4\")\n",
      "\n",
      "     # Define a math problem\n",
      "     math_problem = \"Solve for x in the equation 2x + 5 = 11\"\n",
      "\n",
      "     # Generate the solution\n",
      "     solution = model.invoke(math_problem)\n",
      "\n",
      "     # Print the solution\n",
      "     print(solution)\n",
      "     ```\n",
      "\n",
      "5. **Enhancing with ReAct**:\n",
      "   - Implement the generation of reasoning traces for the math problem to outline the steps leading to the solution.\n",
      "   - If needed, leverage external tools (like a calculator API) to perform specific operations.\n",
      "\n",
      "### Example Implementation\n",
      "\n",
      "- **User Input**: The user asks a math question (e.g., \"What is the integral of x^2?\").\n",
      "- **Break it Down**: The OpenAI LLM identifies smaller steps (like finding the derivative, applying integration rules, etc.).\n",
      "- **Use Python Tools**: The agent uses a Python tool or a calculator API to perform calculations.\n",
      "- **Output**: Finally, the agent summarizes the result and explains the reasoning behind each step.\n",
      "\n",
      "By following this guide, you can successfully create a Math expert React agent that effectively assists users with their mathematical queries, providing clear and structured solutions.\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"messages\": [(\"user\", \"Create a Math expert react agent graph with openai as the llm that breaks down user's math queries into smaller steps and uses python/calculator tools to accomplish them\")]}\n",
    "print_stream(graph.stream(inputs, stream_mode=\"values\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API reference\n",
    "\n",
    "For detailed documentation of all `RAGProRetriever` features and configurations, refer to the [API reference](https://docs.rag.pro/documentation).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
