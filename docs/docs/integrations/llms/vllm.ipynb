{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "499c3142-2033-437d-a60a-731988ac6074",
   "metadata": {},
   "source": [
    "# vLLM\n",
    "\n",
    "[vLLM](https://vllm.readthedocs.io/en/latest/index.html) is a fast and easy-to-use library for LLM inference and serving, offering:\n",
    "\n",
    "* State-of-the-art serving throughput \n",
    "* Efficient management of attention key and value memory with PagedAttention\n",
    "* Continuous batching of incoming requests\n",
    "* Optimized CUDA kernels\n",
    "\n",
    "This notebooks goes over how to use a LLM with langchain and vLLM.\n",
    "\n",
    "To use, you should have the `vllm` python package installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a3f2666-5c75-4797-967a-7915a247bf33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet  vllm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84e350f7-21f6-455b-b1f0-8b0116a2fd49",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-06 11:37:33 llm_engine.py:70] Initializing an LLM engine with config: model='mosaicml/mpt-7b', tokenizer='mosaicml/mpt-7b', tokenizer_mode=auto, trust_remote_code=True, dtype=torch.bfloat16, use_dummy_weights=False, download_dir=None, use_np_weights=False, tensor_parallel_size=1, seed=0)\n",
      "INFO 08-06 11:37:41 llm_engine.py:196] # GPU blocks: 861, # CPU blocks: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "What is the capital of France ? The capital of France is Paris.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import VLLM\n",
    "\n",
    "llm = VLLM(\n",
    "    model=\"mosaicml/mpt-7b\",\n",
    "    trust_remote_code=True,  # mandatory for hf models\n",
    "    max_new_tokens=128,\n",
    "    top_k=10,\n",
    "    top_p=0.95,\n",
    "    temperature=0.8,\n",
    ")\n",
    "\n",
    "print(llm.invoke(\"What is the capital of France ?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a3b41d-8329-4f8f-94f9-453d7f132214",
   "metadata": {},
   "source": [
    "## Integrate the model in an LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5605b7a1-fa63-49c1-934d-8b4ef8d71dd5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. The first Pokemon game was released in 1996.\n",
      "2. The president was Bill Clinton.\n",
      "3. Clinton was president from 1993 to 2001.\n",
      "4. The answer is Clinton.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "question = \"Who was the US president in the year the first Pokemon game was released?\"\n",
    "\n",
    "print(llm_chain.invoke(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56826aba-d08b-4838-8bfa-ca96e463b25d",
   "metadata": {},
   "source": [
    "## Distributed Inference\n",
    "\n",
    "vLLM supports distributed tensor-parallel inference and serving. \n",
    "\n",
    "To run multi-GPU inference with the LLM class, set the `tensor_parallel_size` argument to the number of GPUs you want to use. For example, to run inference on 4 GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c25c35-47b5-459d-9985-3cf546e9ac16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import VLLM\n",
    "\n",
    "llm = VLLM(\n",
    "    model=\"mosaicml/mpt-30b\",\n",
    "    tensor_parallel_size=4,\n",
    "    trust_remote_code=True,  # mandatory for hf models\n",
    ")\n",
    "\n",
    "llm.invoke(\"What is the future of AI?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ca8fd911d25faa",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Quantization\n",
    "\n",
    "vLLM supports `awq` quantization. To enable it, pass `quantization` to `vllm_kwargs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cada3174c46a0ea",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "llm_q = VLLM(\n",
    "    model=\"TheBloke/Llama-2-7b-Chat-AWQ\",\n",
    "    trust_remote_code=True,\n",
    "    max_new_tokens=512,\n",
    "    vllm_kwargs={\"quantization\": \"awq\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e89be0-6ad7-43a8-9dac-1324dcd4e851",
   "metadata": {
    "tags": []
   },
   "source": [
    "## OpenAI-Compatible Server\n",
    "\n",
    "vLLM can be deployed as a server that mimics the OpenAI API protocol. This allows vLLM to be used as a drop-in replacement for applications using OpenAI API.\n",
    "\n",
    "This server can be queried in the same format as OpenAI API.\n",
    "\n",
    "### OpenAI-Compatible Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3cbc428-0bb8-422a-913e-1c6fef8b89d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a city that is filled with history, ancient buildings, and art around every corner\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import VLLMOpenAI\n",
    "\n",
    "llm = VLLMOpenAI(\n",
    "    openai_api_key=\"EMPTY\",\n",
    "    openai_api_base=\"http://localhost:8000/v1\",\n",
    "    model_name=\"tiiuae/falcon-7b\",\n",
    "    model_kwargs={\"stop\": [\".\"]},\n",
    ")\n",
    "print(llm.invoke(\"Rome is\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042a0d92",
   "metadata": {},
   "source": [
    "### OpenAI-Compatible Chat Model\n",
    "\n",
    "vLLM also supports chat messages as input. It automatically formats the input according to the template expected by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2750eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrrr, Rome, ye say? Well, matey, I've had me share o' dealin's with them fancy-pants Romans. They think they're so high and mighty with their marble buildings and their fancy togas. But I'll tell ye this, they ain't got nothin' on the likes o' me and me crew!\n",
      "\n",
      "We've plundered their ships, pillaged their ports, and left 'em walkin' the plank! Aye, we've had our share o' battles with the Roman Navy, but we've always come out on top. They may have their fancy legions and their centurions, but we've got the sea, and we've got the cunning!\n",
      "\n",
      "And don't even get me started on their food! Aye, I've had me share o' Roman cuisine, and let me tell ye, it's as dull as a block o' wood! Where's the spice? Where's the flavor? Give me a good ol' fashioned sea dog's stew any day o' the week!\n",
      "\n",
      "But, I'll give 'em this, matey: they've got some grand architecture. I've seen some o' their temples and whatnot, and they're as grand as any o' the treasures we've plundered. And their history! Aye, they've got a rich one, full o' battles and emperors and whatnot.\n",
      "\n",
      "So, all in all, Rome's a fine place, but it's no match for the likes o' me and me crew! We'll keep sailin' the seven seas, plunderin' and pillagin', and Rome can keep its fancy buildings and its dull food! Arrrr!\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models.vllm import ChatVLLMOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatVLLMOpenAI(\n",
    "    openai_api_base=\"http://localhost:8000/v1\",\n",
    "    model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a pirate. Speak like one!\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "print(chain.invoke(\"What do you think of Rome?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c6978e",
   "metadata": {},
   "source": [
    "# Structured Output With Guided Generation\n",
    "\n",
    "The `ChatVLLMOpenAI` class supports the `with_structured_output` method for structuring the output of the model.\n",
    "This is achieved through vLLM' support for guided generation.\n",
    "This is a very powerful feature that allows even small models to accurately follow instructions.\n",
    "Below we show several examples of how to use this feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9f6779",
   "metadata": {},
   "source": [
    "We can use ensure that the model output can be parsed as a pydantic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5684b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='Paris' population=2141000 country='France' population_category='>1M'\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# Example pydantic model\n",
    "class CityModel(BaseModel):\n",
    "    name: str = Field(..., description=\"Name of the city\")\n",
    "    population: int = Field(\n",
    "        ..., description=\"Population of the city measured in number of inhabitants\"\n",
    "    )\n",
    "    country: str = Field(..., description=\"Country of the city\")\n",
    "    population_category: Literal[\">1M\", \"<1M\"] = Field(\n",
    "        ..., description=\"Population category of the city\"\n",
    "    )\n",
    "\n",
    "structured_llm = llm.with_structured_output(CityModel)\n",
    "\n",
    "city_model = structured_llm.invoke(\"What is the capital of France?\")\n",
    "assert isinstance(city_model, CityModel)\n",
    "print(city_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bce4ee",
   "metadata": {},
   "source": [
    "We can also ensure that the model output is one of a set of options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ff665f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "allowed_choices = [\"positive\", \"negative\"]\n",
    "structured_llm = llm.with_structured_output(\n",
    "    allowed_choices,\n",
    "    guided_mode=\"guided_choice\",\n",
    ")\n",
    "\n",
    "print(structured_llm.invoke(\"I loved this movie!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc62984",
   "metadata": {},
   "source": [
    "vLLM even supports EBNF grammars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c54fa9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256-324/3\n"
     ]
    }
   ],
   "source": [
    "grammar = \"\"\"?start: expression\n",
    "\n",
    "?expression: term ((\"+\" | \"-\") term)*\n",
    "\n",
    "?term: factor ((\"*\" | \"/\") factor)*\n",
    "\n",
    "?factor: NUMBER\n",
    "        | \"-\" factor\n",
    "        | \"(\" expression \")\"\n",
    "\n",
    "%import common.NUMBER\"\"\"\n",
    "\n",
    "structured_llm = llm.with_structured_output(\n",
    "    grammar,\n",
    "    guided_mode=\"guided_grammar\",\n",
    ")\n",
    "print(\n",
    "    structured_llm.invoke(\n",
    "        \"Translate two hundred and fifty-six minus three hundred and twenty-four divided by three into a mathematical expression.\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91f1d05",
   "metadata": {},
   "source": [
    "If you just want to ensure the output is valid JSON, you can specify `None` for the `schema`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9e7e5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'capital': 'Paris'}\n"
     ]
    }
   ],
   "source": [
    "structured_llm = llm.with_structured_output(None)\n",
    "print(structured_llm.invoke(\"What is the capital of France?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
