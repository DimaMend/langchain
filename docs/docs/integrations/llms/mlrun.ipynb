{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLRunLLM: Unleashing the Power of LLMs with MLRun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "[MLRun](https://www.mlrun.org/) is an open-source AI orchestration framework for managing ML and generative AI applications across their lifecycle.\n",
    "\n",
    "Unlock the full potential of your LLMs with MLRun. Our powerful platform streamlines deployment, offering:\n",
    "\n",
    "* Comprehensive Model Tracking: Effortlessly monitor performance, versions, and drifts \n",
    "* Intelligent Optimizations: Automatically boost efficiency and accuracy \n",
    "* Seamless Integration: Fits smoothly into your existing ML workflow\n",
    "* Scalable Architecture: Grow from proof-of-concept to production with ease\n",
    "\n",
    "Plus, enjoy advanced features like:\n",
    "* Real-time monitoring<br>\n",
    "* Automated retraining<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, ensure you have the `mlrun` package installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mlrun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries and set up your environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import mlrun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an MLRun Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize your MLRun project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = mlrun.get_or_create_project(name=\"mlrun-langchain-example\", context=\"./\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Serving Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the serving function for your LLM: <br>\n",
    "(You can create your own or take one from `mlrun.frameworks`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_func = project.set_function(\n",
    "    func=\"llm_model_server.py\",  # Your serving function\n",
    "    name=\"Qwen2-0.5B-Instruct\",  # The name you want to give your function\n",
    "    kind=\"serving\",  # Always serving for this type of functions\n",
    "    image=\"mlrun/mlrun\",  # This is mlrun's default image, you can use your own\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the Model to the Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add your chosen model to the server: <br>\n",
    "(This action depends on your model server and it's parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_func.add_model(\n",
    "    \"huggingface_local_model\",  # Model name\n",
    "    class_name=\"LLMModelServer\",\n",
    "    llm_type=\"HuggingFace\",\n",
    "    model_name=\"Qwen/Qwen2-0.5B-Instruct\",\n",
    "    model_path=\".\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the Model Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy your model server with GPU support:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_func.with_limits(gpus=1)  # If you want to add one GPU\n",
    "server = serving_func.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing and Using MLRunLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that your model is deployed, initialize and use MLRunLLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun_llm_local import Mlrun\n",
    "\n",
    "llm = Mlrun(\n",
    "    server, \"huggingface_local_model\"\n",
    ")  # Give the serving function and the name we gave the deployd model\n",
    "\n",
    "# Example usage\n",
    "response = llm.invoke(\"What is the best jacket to wear with jeans?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates the basics of using MLRun to deploy and interact with an LLM. MLRun's features for model tracking, optimization, and scalability make it an excellent choice for managing LLMs in production environments.\n",
    "\n",
    "For more advanced usage and detailed documentation, visit the [MLRun documentation](https://docs.mlrun.org/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "databricks-summit",
   "language": "python",
   "name": "conda-env-.conda-databricks-summit-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
