{
 "cells": [
  {
   "cell_type": "raw",
   "id": "afaf8039",
   "metadata": {},
   "source": [
    "---\n",
    "sidebar_label: Fireworks\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3d6f34",
   "metadata": {},
   "source": [
    "# FireworksEmbeddings\n",
    "\n",
    "This will help you get started with Fireworks embedding models using LangChain. For detailed documentation on `FireworksEmbeddings` features and configuration options, please refer to the [API reference](https://api.python.langchain.com/en/latest/embeddings/langchain_fireworks.embeddings.FireworksEmbeddings.html).\n",
    "\n",
    "## Overview\n",
    "### Integration details\n",
    "\n",
    "| Class | Package | Local | Serializable | [JS support](https://js.langchain.com/v0.2/docs/integrations/text_embedding/fireworks) | Package downloads | Package latest |\n",
    "| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |\n",
    "| [FireworksEmbeddings](https://api.python.langchain.com/en/latest/embeddings/langchain_fireworks.embeddings.FireworksEmbeddings.html) | [langchain-fireworks](https://api.python.langchain.com/en/latest/fireworks_api_reference.html) | ❌ | ❌ | ✅ | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-fireworks?style=flat-square&label=%20) | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-fireworks?style=flat-square&label=%20) |\n",
    "\n",
    "## Setup\n",
    "\n",
    "To access Fireworks embedding models you'll need to create a Fireworks account, get an API key, and install the `langchain-fireworks` integration package.\n",
    "\n",
    "### Credentials\n",
    "\n",
    "Head to [fireworks.ai](https://fireworks.ai/) to sign up to Fireworks and generate an API key. Once you’ve done this set the FIREWORKS_API_KEY environment variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36521c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"FIREWORKS_API_KEY\"):\n",
    "    os.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass(\"Enter your Fireworks API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84fb993",
   "metadata": {},
   "source": [
    "If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39a4953b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"Enter your LangSmith API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9664366",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "The LangChain Fireworks integration lives in the `langchain-fireworks` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64853226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-fireworks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dd1724",
   "metadata": {},
   "source": [
    "## Instantiation\n",
    "\n",
    "Now we can instantiate our model object and generate chat completions:\n",
    "\n",
    "- TODO: Update model instantiation with relevant params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ea7a09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_fireworks import FireworksEmbeddings\n",
    "\n",
    "embeddings = FireworksEmbeddings(\n",
    "    model=\"nomic-ai/nomic-embed-text-v1.5\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d271b6",
   "metadata": {},
   "source": [
    "## Indexing and Retrieval\n",
    "\n",
    "Embedding models are often used in retrieval-augmented generation (RAG) flows, both as part of indexing data as well as later retrieving it. For more detailed instructions, please see our RAG tutorials under the [working with external knowledge tutorials](/docs/tutorials/#working-with-external-knowledge).\n",
    "\n",
    "Below, see how to index and retrieve data using the `embeddings` object we initialized above. In this example, we will index and retrieve a sample document in the `InMemoryVectorStore`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d817716b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain is the framework for building context-aware reasoning applications'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a vector store with a sample text\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "text = \"LangChain is the framework for building context-aware reasoning applications\"\n",
    "\n",
    "vectorstore = InMemoryVectorStore.from_texts(\n",
    "    [text],\n",
    "    embedding=embeddings,\n",
    ")\n",
    "\n",
    "# Use the vectorstore as a retriever\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Retrieve the most similar text\n",
    "retrieved_documents = retriever.invoke(\"What is LangChain?\")\n",
    "\n",
    "# show the retrieved document's content\n",
    "retrieved_document[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02b9855",
   "metadata": {},
   "source": [
    "## Direct Usage\n",
    "\n",
    "Under the hood, the vectorstore and retriever implementations are calling `embeddings.embed_documents(...)` and `embeddings.embed_query(...)` to create embeddings for the text(s) used in `from_texts` and retrieval `invoke` operations, respectively.\n",
    "\n",
    "You can directly call these methods to get embeddings for your own use cases.\n",
    "\n",
    "### Embed single texts\n",
    "\n",
    "You can embed single texts or documents with `embed_query`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d2befcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01666259765625, 0.011688232421875, -0.1181640625, -0.10205078125, 0.05438232421875, -0.0890502929\n"
     ]
    }
   ],
   "source": [
    "single_vector = embeddings.embed_query(text)\n",
    "print(str(single_vector)[:100])  # Show the first 100 characters of the vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5a7d03",
   "metadata": {},
   "source": [
    "### Embed multiple texts\n",
    "\n",
    "You can embed multiple texts with `embed_documents`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f4d6e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.016632080078125, 0.01165008544921875, -0.1181640625, -0.10186767578125, 0.05438232421875, -0.0890\n",
      "[-0.02667236328125, 0.036651611328125, -0.1630859375, -0.0904541015625, -0.022430419921875, -0.09545\n"
     ]
    }
   ],
   "source": [
    "text2 = (\n",
    "    \"LangGraph is a library for building stateful, multi-actor applications with LLMs\"\n",
    ")\n",
    "two_vectors = embeddings.embed_documents([text, text2])\n",
    "for vector in two_vectors:\n",
    "    print(str(vector)[:100])  # Show the first 100 characters of the vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98785c12",
   "metadata": {},
   "source": [
    "### Async Usage\n",
    "\n",
    "You can also use `aembed_query` and `aembed_documents` for producing embeddings asynchronously:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c3bef91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01666259765625, 0.011688232421875, -0.1181640625, -0.10205078125, 0.05438232421875, -0.0890502929\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "async def async_example():\n",
    "    single_vector = await embeddings.aembed_query(text)\n",
    "    print(str(single_vector)[:100])  # Show the first 100 characters of the vector\n",
    "\n",
    "\n",
    "if asyncio.get_event_loop().is_running():\n",
    "    # This branch is used when in a jupyter notebook.\n",
    "    # Which already has a running event loop!\n",
    "    await async_example()\n",
    "else:\n",
    "    # This code path is used when testing from\n",
    "    # python or ipython interpreters that do not start\n",
    "    # an event loop by default.\n",
    "\n",
    "    asyncio.run(async_example())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
