{
 "cells": [
  {
   "cell_type": "raw",
   "id": "eb65deaa",
   "metadata": {},
   "source": [
    "---\n",
    "sidebar_label: vLLM Chat\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7e5679-aa06-47e4-a1a3-b6b70e604017",
   "metadata": {},
   "source": [
    "# vLLM Chat\n",
    "\n",
    "vLLM can be deployed as a server that mimics the OpenAI API protocol. This allows vLLM to be used as a drop-in replacement for applications using OpenAI API. This server can be queried in the same format as OpenAI API.\n",
    "\n",
    "This notebook covers how to get started with vLLM chat models using langchain's `ChatOpenAI` **as it is**, as well as how to do structured generation with `ChatVLLMOpenAI`.\n",
    "\n",
    "We assume you already have a vLLM server running. See the [vLLM README](https://docs.vllm.ai/en/latest/serving/deploying_with_docker.html) for instructions on how to deploy a vLLM server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "060a2e3d-d42f-4221-bd09-a9a06544dcd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_community.chat_models import ChatVLLMOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf24d732-68a9-44fd-b05d-4903ce5620c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The URL of the vLLM inference server\n",
    "inference_server_url = \"http://10.1.0.255:8000/v1\"\n",
    "\n",
    "llm = ChatVLLMOpenAI(\n",
    "    model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    openai_api_base=inference_server_url,\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7c17277",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# Example pydantic model\n",
    "class CityModel(BaseModel):\n",
    "    name: str = Field(..., description=\"Name of the city\")\n",
    "    population: int = Field(\n",
    "        ..., description=\"Population of the city measured in number of inhabitants\"\n",
    "    )\n",
    "    country: str = Field(..., description=\"Country of the city\")\n",
    "    population_category: Literal[\">1M\", \"<1M\"] = Field(\n",
    "        ..., description=\"Population category of the city\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c20bfa69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CityModel(name='What is the capital of France?Paris has 10 million citizens', population=10000000, country='France', population_category='>1M')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.with_structured_output(CityModel, instructions=\"Paris has 10 million citizens\").invoke(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10059860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CityModel(name='Paris', population=2140000, country='France', population_category='>1M')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.with_structured_output(CityModel, instructions=\"\").invoke(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64baf099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': '',\n",
       " 'additional_kwargs': {'tool_calls': [{'id': 'chatcmpl-tool-b9d7f5a2d56a4968bd0ce428e2444ceb',\n",
       "    'function': {'arguments': '{\"name\":\"What is the capital of France?\",\"population\":10000000,\"country\":\"France\",\"population_category\":\">1M\"}',\n",
       "     'name': 'CityModel'},\n",
       "    'type': 'function'}]},\n",
       " 'response_metadata': {'token_usage': {'completion_tokens': 31,\n",
       "   'prompt_tokens': 23,\n",
       "   'total_tokens': 54},\n",
       "  'model_name': 'meta-llama/Meta-Llama-3-8B-Instruct',\n",
       "  'system_fingerprint': None,\n",
       "  'finish_reason': 'stop',\n",
       "  'logprobs': None},\n",
       " 'type': 'ai',\n",
       " 'name': None,\n",
       " 'id': 'run-bcc2eed6-80eb-43a5-8330-0282a715e8f6-0',\n",
       " 'example': False,\n",
       " 'tool_calls': [{'name': 'CityModel',\n",
       "   'args': {'name': 'What is the capital of France?',\n",
       "    'population': 10000000,\n",
       "    'country': 'France',\n",
       "    'population_category': '>1M'},\n",
       "   'id': 'chatcmpl-tool-b9d7f5a2d56a4968bd0ce428e2444ceb'}],\n",
       " 'invalid_tool_calls': [],\n",
       " 'usage_metadata': {'input_tokens': 23,\n",
       "  'output_tokens': 31,\n",
       "  'total_tokens': 54}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "tool_llm = llm.bind_tools([CityModel], tool_choice=\"CityModel\")\n",
    "res = tool_llm.invoke(\"What is the capital of France?\\nParis has 10 million citizens\")\n",
    "json.loads(res.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90cd149d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ \n",
      "\"name\" \t: \t\"Paris\" \t, \n",
      "\"population\" \t: \t10000000 \t, \n",
      "\"country\" \t: \t\"France\" \n",
      "}\n"
     ]
    }
   ],
   "source": [
    "tool_llm = llm.bind(response_format={\"type\": \"json_object\"})\n",
    "res = tool_llm.invoke(\"\"\"\n",
    "Create a json object with the following fields:\n",
    "- name: Paris\n",
    "- population: 10 million\n",
    "- country: France\n",
    "\"\"\")\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d39edc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
