{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatOutlines\n",
    "\n",
    "[Outlines](https://github.com/outlines-dev/outlines) is a library for constrained language generation. It allows you to use large language models (LLMs) with various backends while applying constraints to the generated output.\n",
    "\n",
    "## Overview\n",
    "\n",
    "### Integration details\n",
    "\n",
    "| Class | Package | Local | Serializable | JS support |\n",
    "| :--- | :--- | :---: | :---: |  :---: |\n",
    "| [ChatOutlines](https://python.langchain.com/docs/integrations/chat/outlines) | [langchain-community](https://python.langchain.com/docs/integrations/chat/outlines) | ✅ | ❌ | ❌ |\n",
    "\n",
    "### Model features\n",
    "\n",
    "| [Tool calling](/docs/how_to/tool_calling) | [Structured output](/docs/how_to/structured_output/) | JSON mode | Image input | Audio input | Video input | [Token-level streaming](/docs/how_to/chat_streaming/) | Native async | [Token usage](/docs/how_to/chat_token_usage_tracking/) | [Logprobs](/docs/how_to/logprobs/) | Grammars |\n",
    "| :---: | :---: | :---: | :---: |  :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n",
    "| ✅ | ✅ | ✅ | ✅ | ❌ | ❌ | ✅ | ❌ | ❌ | ❌ | ✅ | \n",
    "\n",
    "## Setup\n",
    "\n",
    "First, you need to install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pip install langchain-community outlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiation\n",
    "\n",
    "You can instantiate the ChatOutlines model with various backends:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models.outlines import ChatOutlines\n",
    "\n",
    "# For llamacpp backend\n",
    "model = ChatOutlines(model=\"TheBloke/phi-2-GGUF/phi-2.Q4_K_M.gguf\", backend=\"llamacpp\")\n",
    "\n",
    "# For vllm backend (not available on Mac)\n",
    "model = ChatOutlines(model=\"meta-llama/Llama-3.2-1B\", backend=\"vllm\")\n",
    "\n",
    "# For mlxlm backend (only available on Mac)\n",
    "model = ChatOutlines(model=\"mistralai/Ministral-8B-Instruct-2410\", backend=\"mlxlm\")\n",
    "\n",
    "# For huggingface ransformers backend\n",
    "model = ChatOutlines(model=\"microsoft/phi-2\")  # defaults to transformers backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Usage\n",
    "\n",
    "You can use the ChatOutlines model for simple chat completions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "messages = [HumanMessage(content=\"What will the capital of mars be called?\")]\n",
    "response = model.invoke(messages)\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming\n",
    "\n",
    "ChatOutlines supports streaming of tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"Count to 10 in French:\")]\n",
    "\n",
    "for chunk in model.stream(messages):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constrained Generation\n",
    "\n",
    "ChatOutlines allows you to apply various constraints to the generated output:\n",
    "\n",
    "### Regex Constraint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.regex = r\"((25[0-5]|2[0-4]\\d|[01]?\\d\\d?)\\.){3}(25[0-5]|2[0-4]\\d|[01]?\\d\\d?)\"\n",
    "\n",
    "response = model.invoke(\"What is the IP address of Google's DNS server?\")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.type_constraints = int\n",
    "response = model.invoke(\"What is the answer to life, the universe, and everything?\")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pydantic and JSON Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Person(BaseModel):\n",
    "    name: str\n",
    "\n",
    "\n",
    "model.json_schema = Person\n",
    "response = model.invoke(\"Who are the main contributors to LangChain?\")\n",
    "person = Person.model_validate_json(response.content)\n",
    "\n",
    "person"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Free Grammars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.grammar = \"\"\"\n",
    "?start: expression\n",
    "?expression: term ((\"+\" | \"-\") term)\n",
    "?term: factor ((\"\" | \"/\") factor)\n",
    "?factor: NUMBER | \"-\" factor | \"(\" expression \")\"\n",
    "%import common.NUMBER\n",
    "%import common.WS\n",
    "%ignore WS\n",
    "\"\"\"\n",
    "response = model.invoke(\"Give me a complex arithmetic expression:\")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## LangChain's Structured Output\n",
    "\n",
    "You can also use LangChain's Structured Output with ChatOutlines:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class AnswerWithJustification(BaseModel):\n",
    "    answer: str\n",
    "    justification: str\n",
    "\n",
    "\n",
    "_model = model.with_structured_output(AnswerWithJustification)\n",
    "result = _model.invoke(\"What weighs more, a pound of bricks or a pound of feathers?\")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Outlines Documentation: \n",
    "\n",
    "https://dottxt-ai.github.io/outlines/latest/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
