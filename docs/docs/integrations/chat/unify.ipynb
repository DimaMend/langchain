{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e49f1e0d",
   "metadata": {},
   "source": [
    "# ChatUnify\n",
    "\n",
    "This notebook covers how to get started with Unify chat models.\n",
    "\n",
    "[Unify](https://unify.ai/hub) dynamically routes each query to the best LLM, with support for providers such as OpenAI, MistralAI, Perplexity AI, and Together AI. You can also access all providers individually using a single API key.\n",
    "\n",
    "You can check out our [live benchmarks](https://unify.ai/hub/mixtral-8x7b-instruct-v0.1) to see where the data is coming from!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b31fc49",
   "metadata": {},
   "source": [
    "\n",
    "## Installation\n",
    "\n",
    "First thing to do is installing the `LangChain x Unify` and `LangChain Core` packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c3bef91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-core in /opt/anaconda3/lib/python3.12/site-packages (0.3.5)\n",
      "Collecting langchain_community\n",
      "  Downloading langchain_community-0.3.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core) (0.1.125)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core) (23.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core) (2.9.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core) (8.2.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core) (4.11.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/anaconda3/lib/python3.12/site-packages (from langchain_community) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from langchain_community) (3.9.5)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting langchain<0.4.0,>=0.3.0 (from langchain_community)\n",
      "  Downloading langchain-0.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain_community) (1.26.4)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
      "  Downloading pydantic_settings-2.5.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from langchain_community) (2.32.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.3)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core) (2.1)\n",
      "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain<0.4.0,>=0.3.0->langchain_community)\n",
      "  Downloading langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core) (3.10.7)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (2.23.4)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (0.21.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2->langchain_community) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2->langchain_community) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2->langchain_community) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2->langchain_community) (2024.8.30)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core) (1.0.2)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core) (0.14.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
      "Downloading langchain_community-0.3.0-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading langchain-0.3.0-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_settings-2.5.2-py3-none-any.whl (26 kB)\n",
      "Downloading langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\n",
      "Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Installing collected packages: typing-inspect, marshmallow, dataclasses-json, pydantic-settings, langchain-text-splitters, langchain, langchain_community\n",
      "Successfully installed dataclasses-json-0.6.7 langchain-0.3.0 langchain-text-splitters-0.3.0 langchain_community-0.3.0 marshmallow-3.22.0 pydantic-settings-2.5.2 typing-inspect-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U unifyai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4f3e15",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Make sure to set the `UNIFY_KEY` environment variable. You can get a key in the [Unify Console](https://console.unify.ai/login)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6e3e2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"UNIFY_KEY\"] = \"API_KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b67a4e",
   "metadata": {},
   "source": [
    "## Usage \n",
    "\n",
    "Let's take a look at how to use the package now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d53c52c",
   "metadata": {},
   "source": [
    "The first thing we can do is initialize a model. To configure Unify's router, pass an endpoint string to `ChatUnify`. You can read more about this in [Unify's docs](https://unify.ai/docs/hub/concepts/runtime_routing.html).\n",
    "\n",
    "In this case, we will use the cheapest endpoint for `llama2-70b` in terms of input cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d44e6c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ChatUnify' from 'langchain_community.chat_models' (/opt/anaconda3/lib/python3.12/site-packages/langchain_community/chat_models/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatUnify\n\u001b[1;32m      3\u001b[0m chat \u001b[38;5;241m=\u001b[39m ChatUnify(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama-2-70b-chat@input-cost\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'ChatUnify' from 'langchain_community.chat_models' (/opt/anaconda3/lib/python3.12/site-packages/langchain_community/chat_models/__init__.py)"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatUnify\n",
    "\n",
    "chat = ChatUnify(model=\"gpt-3.5-turbo@openai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0e825e",
   "metadata": {},
   "source": [
    "Once we have initialized the model, we can query it with `invoke`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b4257e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"  Hello! I'm doing well, thanks for asking. I'm a large language model, so I don't have feelings like humans do, but I'm always happy to chat with you. How about you? How's your day going?\", response_metadata={'usage': {'completion_tokens': 54, 'prompt_tokens': 14, 'total_tokens': 68, 'cost': 0.000111}, 'model': 'llama-2-70b-chat@input-cost', 'finish_reason': 'stop'})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.invoke(\"Hello! How are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be31767c",
   "metadata": {},
   "source": [
    "### Single Sign-On"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e103a7",
   "metadata": {},
   "source": [
    "If you don't want the router to select the provider, you can also use our SSO to query endpoints in different providers without making accounts with all of them. For example, all of these are valid endpoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28af71e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatUnify(model=\"llama-2-70b-chat@together-ai\")\n",
    "chat = ChatUnify(model=\"gpt-3.5-turbo@openai\")\n",
    "chat = ChatUnify(model=\"mixtral-8x7b-instruct-v0.1@mistral-ai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fb201e",
   "metadata": {},
   "source": [
    "This allows you to quickly switch and test different models and providers. For example, if you are working on an application that uses gpt-4 under the hood, you can use this to query a much cheaper LLM during development and/or testing to reduce costs.\n",
    "\n",
    "Take a look at the available ones [here](https://unify.ai/hub)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2ef135",
   "metadata": {},
   "source": [
    "### Chaining Inputs\n",
    "\n",
    "Let's build a simple chain that leverages prompt templates now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e387d30",
   "metadata": {},
   "source": [
    "We will need to define a prompt template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62e0dbc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant that translates English to French.\"),\n",
    "        (\"human\", \"Translate this sentence from English to French. {english_text}.\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d5ecff",
   "metadata": {},
   "source": [
    "And then simply build and invoke the resulting chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "415182fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='  Sure, I\\'d be happy to help! Here\\'s the translation of \"Hello! How are you?\" from English to French:\\n\\nBonjour! Comment allez-vous?\\n\\nHere\\'s a breakdown of the translation:\\n\\n* \"Hello\" becomes \"Bonjour\" in French.\\n* \"How are you?\" becomes \"Comment allez-vous?\" in French. The word \"comment\" means \"how,\" and \"allez-vous\" is a phrase that means \"are you.\"\\n\\nSo, the full translation is \"Bonjour! Comment allez-vous?\" which means \"Hello! How are you?\" in French.', response_metadata={'usage': {'completion_tokens': 145, 'prompt_tokens': 48, 'total_tokens': 193, 'cost': 0.00030429999999999997}, 'model': 'llama-2-70b-chat@input-cost', 'finish_reason': 'stop'})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_unify.chat_models import ChatUnify\n",
    "chat = ChatUnify(model=\"llama-2-70b-chat@input-cost\")\n",
    "chain = prompt | chat\n",
    "chain.invoke({\"english_text\": \"Hello! How are you?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bb9a47",
   "metadata": {},
   "source": [
    "### Streaming and optimizing for latency\n",
    "\n",
    "If you are building an application where responsiveness is key, you most likely want to get a streaming response. On top of that, ideally you would use the provider with the lowest Time to First Token, to reduce the time your users are waiting for a response. Using Unify this would look something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c938a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A large language model is a type of artificial intelligence (AI) model that is designed to understand and generate human-like text. It is called \"large\" because it requires a substantial amount of data and computational resources to train, which in turn enables it to capture a broad range of linguistic patterns and relationships.\n",
      "\n",
      "Language models learn to predict the next word in a sequence based on the context of the previous words. They are trained on vast amounts of text data, such as books, websites, and other written materials. During training, the model learns the statistical patterns and relationships between words and sentences, enabling it to generate coherent and contextually relevant text.\n",
      "\n",
      "Large language models have achieved impressive results in various natural language processing tasks, such as text completion, translation, summarization, and chatbots. They have been shown to generate creative and sometimes surprising text, and they have also been used to develop more advanced AI systems that can understand and respond to human speech and text.\n",
      "\n",
      "These models can be particularly useful in applications where humans and machines need to communicate and understand each other naturally, such as virtual assistants, customer service chatbots, and language translation software. However, they also raise important ethical concerns related to privacy, bias, and the potential misuse of AI technology."
     ]
    }
   ],
   "source": [
    "chat_ttft = ChatUnify(model=\"mistral-7b-instruct-v0.2@ttft\")\n",
    "for chunk in chat_ttft.stream(\"What is a large language model?\"):\n",
    "    print(chunk.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f295f94",
   "metadata": {},
   "source": [
    "### Batching and Lowest Output Cost\n",
    "\n",
    "On the other hand, maybe you are building an AI service that processes inputs in batches to generate content. In this case, you may want to get the cheaper provider for longer outputs. Let's see how you can do this using `batch` and dynamic routing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61835218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content=\"  Rome, the Eternal City, is a must-visit destination for any traveler. With a rich history spanning over 2,000 years, Rome is home to an array of iconic landmarks, world-class museums, and a vibrant culinary scene. Whether you're interested in history, art, architecture, or food, Rome has something for everyone.\\n\\nOne of the most iconic landmarks in Rome is the Colosseum, also known as the Flavian Amphitheatre. This ancient structure was built in 80 AD and could hold up to 50,000 spectators. The Colosseum was used for gladiatorial contests, animal hunts, and theatrical performances. Today, it is a UNESCO World Heritage Site and a symbol of Rome's rich history.\\n\\nAnother famous landmark in Rome is the Pantheon, a temple built in 126 AD. The Pantheon is considered one of the greatest architectural achievements of all time and is still standing after nearly 2,000 years. Its impressive dome, which was the largest in the world for over 1,000 years, is a testament to the ingenuity of the ancient Romans.\\n\\nRome is also home to an array of world-class museums. The Vatican Museums, which house some of the most famous art in the world, including Michelangelo's Sistine Chapel ceiling, are a must-visit. The Galleria Borghese, which houses an impressive collection of Renaissance and Baroque art, is another museum not to be missed.\\n\\nIn addition to its landmarks and museums, Rome is also famous for its culinary scene. From pizza to pasta, Rome is a food lover's paradise. Be sure to try some of the local specialties, such as carbonara, amatriciana, and cacio e pepe, all of which are pasta dishes made with guanciale (cured pork jowl), eggs, and Pecorino Romano cheese. And of course, no visit to Rome is complete without trying a slice (or two) of pizza.\\n\\nRome is also a city of beauty, with its picturesque streets, charming piazzas, and stunning architecture. The Trevi Fountain, a beautiful baroque fountain built in 1762, is a popular spot for tourists and locals alike. The Spanish Steps, a grand staircase built in 1725, is another famous landmark that offers beautiful views of the city.\\n\\nAt night, Rome comes alive with a vibrant nightlife. From bars to clubs to live music venues, there's something for everyone. The Testaccio neighborhood, for example, is known for its lively bars and nightlife, while the Trastevere neighborhood is famous for its live music venues and romantic atmosphere.\\n\\nIn conclusion, Rome is a city that has something for everyone. Whether you're interested in history, art, architecture, food, or nightlife, Rome is a destination that is sure to leave a lasting impression. With its rich history, stunning landmarks, world-class museums, and delicious culinary scene, Rome is a city that should be on every traveler's bucket list. So, pack your bags and get ready to experience the Eternal City for yourself!\", response_metadata={'usage': {'completion_tokens': 763, 'prompt_tokens': 14, 'total_tokens': 777, 'cost': 0.000777}, 'model': 'llama-2-70b-chat@houtput-cost', 'finish_reason': 'stop'}),\n",
       " AIMessage(content=\"  Ah, Paris – the City of Light, the City of Love, the City of Dreams. For centuries, Paris has been captivating the hearts of artists, writers, and romantics alike. Its charm, beauty, and history have made it one of the most popular tourist destinations in the world.\\n\\nAs soon as you step foot in Paris, you can feel the magic in the air. The city is filled with stunning architecture, from the iconic Eiffel Tower to the grand Notre-Dame Cathedral. The Seine River, which runs through the heart of the city, offers breathtaking views of the city's landmarks and bridges. And, of course, there are the famous Parisian cafes, where you can sit and watch the world go by while sipping on a croissant and a cup of coffee.\\n\\nOne of the must-see landmarks in Paris is the Eiffel Tower. Built for the World Exposition in 1889, it stands over 1,000 feet tall and offers panoramic views of the city. You can take the elevator to the top for a stunning view of the city, or enjoy a romantic dinner at the tower's restaurant, Le Jules Verne.\\n\\nAnother iconic landmark in Paris is the Notre-Dame Cathedral. This beautiful Gothic church has been standing for over 800 years and is one of the most famous cathedrals in the world. Its intricate stone carvings, stained glass windows, and majestic organ make it a must-visit attraction for anyone visiting Paris.\\n\\nBut Paris is not just about its landmarks – it's also about its art and culture. The city is home to some of the world's most famous museums, including the Louvre and the Musée d'Orsay. The Louvre is home to an impressive collection of art and artifacts from around the world, including the Mona Lisa, Venus de Milo, and the Winged Victory of Samothrace. The Musée d'Orsay, on the other hand, is dedicated to Impressionist and Post-Impressionist art, with works by famous artists such as Monet, Renoir, and Van Gogh.\\n\\nAnd then, of course, there's the food. French cuisine is famous the world over, and Paris is the perfect place to indulge in all the delicious flavors and pastries that France has to offer. From croissants and baguettes to escargot and coq au vin, there's something for every palate in Paris. And don't forget to try some of the city's famous desserts, such as macarons, éclairs, and tarte tatin.\\n\\nBut Paris isn't just about the food – it's also about the fashion. The city is home to some of the world's most famous fashion designers, including Chanel, Dior, and Yves Saint-Laurent. And, of course, there's the famous Galeries Lafayette department store, where you can find the latest fashions and trends.\\n\\nIn the evening, Paris comes alive with music and entertainment. The city is home to many famous cabarets, such as the Moulin Rouge and the Lido de Paris, where you can enjoy a night of dance, music, and spectacle. And, of course, there are the famous Parisian nightclubs, where you can dance the night away under the sparkling lights of the city.\\n\\nIn short, Paris is a city that has something for everyone. Whether you're interested in history, art, fashion, food, or entertainment, Paris has it all. It's a city that will capture your heart and leave you with memories that will last a lifetime. So, if you haven't been to Paris yet, what are you waiting for? Book your trip today and discover the magic of the City of Light for yourself.\", response_metadata={'usage': {'completion_tokens': 870, 'prompt_tokens': 14, 'total_tokens': 884, 'cost': 0.000884}, 'model': 'llama-2-70b-chat@houtput-cost', 'finish_reason': 'stop'})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    \"Write a blog post about Rome\",\n",
    "    \"Write a blog post about Paris\"\n",
    "]\n",
    "\n",
    "chat_fastest = ChatUnify(model=\"llama-2-70b-chat@houtput-cost\")\n",
    "chat_fastest.batch(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e711e6d2",
   "metadata": {},
   "source": [
    "### Async calls and Lowest Input Cost\n",
    "\n",
    "Last but not least, you can also run request asynchronously. For tasks like long document summarization, optimizing for input costs is crucial. Unify's dynamic router can do this too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "356e71cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content=\" OpenAI: Pioneering 'safe' artificial general intelligence.\", response_metadata={'usage': {'completion_tokens': 14, 'prompt_tokens': 133, 'total_tokens': 147, 'cost': 3.969000000000001e-05}, 'model': 'mixtral-8x7b-instruct-v0.1@input-cost', 'finish_reason': 'stop'}),\n",
       " AIMessage(content=' Mistral AI: French startup producing open-source AI language models, raised $2bn+.', response_metadata={'usage': {'completion_tokens': 21, 'prompt_tokens': 158, 'total_tokens': 179, 'cost': 4.833e-05}, 'model': 'mixtral-8x7b-instruct-v0.1@input-cost', 'finish_reason': 'stop'}),\n",
       " AIMessage(content=' Meta releases large language model LLaMA; weights leaked to public within a week.', response_metadata={'usage': {'completion_tokens': 19, 'prompt_tokens': 227, 'total_tokens': 246, 'cost': 6.642e-05}, 'model': 'mixtral-8x7b-instruct-v0.1@input-cost', 'finish_reason': 'stop'})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    \"Summarize this in 10 words or less. OpenAI is a U.S. based artificial intelligence \"\n",
    "    \"(AI) research organization founded in December 2015, researching artificial intelligence \"\n",
    "    \"with the goal of developing 'safe and beneficial' artificial general intelligence, \"\n",
    "    \"which it defines as 'highly autonomous systems that outperform humans at most economically \"\n",
    "    \"valuable work'. As one of the leading organizations of the AI spring, it has developed \"\n",
    "    \"several large language models, advanced image generation models, and previously, released \"\n",
    "    \"open-source models. Its release of ChatGPT has been credited with starting the AI spring\", \n",
    "\n",
    "    \"Summarize this in 10 words or less. Mistral AI is a French company selling\"\n",
    "    \" artificial intelligence (AI) products. \"\n",
    "    \"It was founded in April 2023 by previous employees of Meta Platforms and Google DeepMind. \"\n",
    "    \"The company raised €385 million in October 2023 and in December 2023 it was valued at \"\n",
    "    \"more than $2 billion. It produces open source large language models, citing the \"\n",
    "    \"foundational importance of open-source software, and as a response to proprietary models. \"\n",
    "    \"As of March 2024, two models have been published and are available as weights. \"\n",
    "    \"Three more models, Small, Medium and Large, are available via API only.\", \n",
    "\n",
    "    \"Summarize this in 10 words or less. LLaMA (Large Language Model Meta AI) is a family of\"\n",
    "    \" autoregressive large language models (LLMs), \"\n",
    "    \"released by Meta AI starting in February 2023. For the first version of LLaMA, four model sizes \"\n",
    "    \"were trained: 7, 13, 33, and 65 billion parameters. LLaMA's developers reported that the 13B \"\n",
    "    \"parameter model's performance on most NLP benchmarks exceeded that of the much larger GPT-3 \"\n",
    "    \"(with 175B parameters) and that the largest model was competitive with state of the art models \"\n",
    "    \"such as PaLM and Chinchilla. Whereas the most powerful LLMs have generally been accessible only \"\n",
    "    \"through limited APIs (if at all), Meta released LLaMA's model weights to the research community \"\n",
    "    \"under a noncommercial license. Within a week of LLaMA's release, its weights were leaked to the \"\n",
    "    \"public on 4chan via BitTorrent.\"\n",
    "]\n",
    "\n",
    "chat_model = ChatUnify(model=\"mixtral-8x7b-instruct-v0.1@input-cost\")\n",
    "\n",
    "\n",
    "await chat_model.abatch(messages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
