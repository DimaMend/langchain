{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "683953b3",
   "metadata": {},
   "source": [
    "# Pinecone\n",
    "\n",
    ">[Pinecone](https://docs.pinecone.io/docs/overview) is a vector database with broad functionality.\n",
    "\n",
    "This notebook shows how to use functionality related to the `Pinecone` vector database.\n",
    "\n",
    "## Setup\n",
    "\n",
    "To use the `PineconeVectorStore` you first need to install the partner package, as well as the other packages used throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c41cad-08ef-4f72-a545-2151e4598efe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -qU langchain-pinecone pinecone-notebooks langchain-ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1917d123",
   "metadata": {},
   "source": [
    "Migration note: if you are migrating from the `langchain_community.vectorstores` implementation of Pinecone, you may need to remove your `pinecone-client` v2 dependency before installing `langchain-pinecone`, which relies on `pinecone-client` v3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6dc4de",
   "metadata": {},
   "source": [
    "### Credentials\n",
    "\n",
    "Create a new Pinecone account, or sign into your existing one, and create an API key to use in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb554814",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "import time\n",
    "\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "if not os.getenv(\"PINECONE_API_KEY\"):\n",
    "    os.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Enter your Pinecone API key: \")\n",
    "\n",
    "pinecone_api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef1d828",
   "metadata": {},
   "source": [
    "If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23b5ac5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\"Enter your LangSmith API key: \")\n",
    "# os.environ[\"LANGSMITH_TRACING\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658706a3",
   "metadata": {},
   "source": [
    "## Instantiation\n",
    "\n",
    "Before initializing our vector store, let's connect to a Pinecone index. If one named `index_name` doesn't exist, it will be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "276a06dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "index_name = \"langchain-index\"  # change if desired\n",
    "\n",
    "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "\n",
    "if index_name not in existing_indexes:\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=4096,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "    )\n",
    "    while not pc.describe_index(index_name).status[\"ready\"]:\n",
    "        time.sleep(1)\n",
    "\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4d377f",
   "metadata": {},
   "source": [
    "Now that our Pinecone index is setup, we can initialize our vector store. We are using the `langchain-ollama` embeddings since it does not require payment or an API key. For information on `OllamaEmbeddings` read [this page](https://python.langchain.com/v0.2/docs/integrations/text_embedding/ollama/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e104aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embedding_function= OllamaEmbeddings(model=\"llama3\")\n",
    "\n",
    "vector_store = PineconeVectorStore(index=index, embedding=embedding_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48721e29",
   "metadata": {},
   "source": [
    "## Manage vector store\n",
    "\n",
    "### Add items to vector store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70e688f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '2', '3']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "document_1 = Document(\n",
    "    page_content=\"foo\",\n",
    "    metadata={\"source\": \"https://example.com\"}\n",
    ")\n",
    "\n",
    "document_2 = Document(\n",
    "    page_content=\"bar\",\n",
    "    metadata={\"source\": \"https://another-example.com\"}\n",
    ")\n",
    "\n",
    "document_3 = Document(\n",
    "    page_content=\"baz\",\n",
    "    metadata={\"source\": \"https://example.com\"}\n",
    ")\n",
    "\n",
    "documents = [document_1, document_2, document_3]\n",
    "\n",
    "vector_store.add_documents(documents=documents,ids=[\"1\",\"2\",\"3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120922b3",
   "metadata": {},
   "source": [
    "### Delete items from vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b8437cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.delete(ids=[\"3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee21c89",
   "metadata": {},
   "source": [
    "## Query vector store\n",
    "\n",
    "Once your vector store has been created and the relevant documents have been added you will most likely wish to query it during the running of your chain or agent. \n",
    "\n",
    "### Query directly\n",
    "\n",
    "Performing a simple similarity search can be done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ffbcb3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* bar [{'source': 'https://another-example.com'}]\n"
     ]
    }
   ],
   "source": [
    "results = vector_store.similarity_search(query=\"thud\",k=1,filter={\"source\":\"https://another-example.com\"})\n",
    "for doc in results:\n",
    "    print(f\"* {doc.page_content} [{doc.metadata}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f3494d",
   "metadata": {},
   "source": [
    "If you want to execute a similarity search and receive the corresponding scores you can run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5fb24583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* [SIM=0.425751] foo [{'source': 'https://example.com'}]\n"
     ]
    }
   ],
   "source": [
    "results = vector_store.similarity_search_with_score(query=\"thud\",k=1,filter={\"source\":\"https://example.com\"})\n",
    "for doc, score in results:\n",
    "    print(f\"* [SIM={score:3f}] {doc.page_content} [{doc.metadata}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1855941b",
   "metadata": {},
   "source": [
    "There are more search methods (such as MMR) not listed in this notebook, to find all of them be sure to read the [API reference](https://api.python.langchain.com/en/latest/vectorstores/langchain_pinecone.vectorstores.PineconeVectorStore.html).\n",
    "\n",
    "### Query by turning into retriever\n",
    "\n",
    "You can also transform the vector store into a retriever for easier usage in your chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78140e87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://another-example.com'}, page_content='bar')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\": 1}\n",
    ")\n",
    "retriever.invoke(\"thud\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c6cbf1",
   "metadata": {},
   "source": [
    "Using retriever in a simple RAG chain:\n",
    "\n",
    "### WARNING\n",
    "\n",
    "To run this cell, you need to be able to use the `ChatOpenAI` chat model. Learn about how to set it up by visiting [this page](https://python.langchain.com/v0.2/docs/integrations/chat/openai/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e587f853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, I don't have enough information to answer that question.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"thud\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5722bc",
   "metadata": {},
   "source": [
    "## API reference\n",
    "\n",
    "For detailed documentation of all __ModuleName__VectorStore features and configurations head to the API reference: https://api.python.langchain.com/en/latest/vectorstores/langchain_pinecone.vectorstores.PineconeVectorStore.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
