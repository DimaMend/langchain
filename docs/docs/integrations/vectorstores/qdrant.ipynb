{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "683953b3",
   "metadata": {},
   "source": [
    "# Qdrant\n",
    "\n",
    ">[Qdrant](https://qdrant.tech/documentation/) (read: quadrant ) is a vector similarity search engine. It provides a production-ready service with a convenient API to store, search, and manage vectors with additional payload and extended filtering support. It makes it useful for all sorts of neural network or semantic-based matching, faceted search, and other applications.\n",
    "\n",
    "This documentation demonstrates how to use Qdrant with Langchain for dense/sparse and hybrid retrieval.\n",
    "\n",
    "> This page documents the `QdrantVectorStore` class that supports multiple retrieval modes via Qdrant's new [Query API](https://qdrant.tech/blog/qdrant-1.10.x/). It requires you to run Qdrant v1.10.0 or above.\n",
    "\n",
    "\n",
    "## Setup\n",
    "\n",
    "There are various modes of how to run `Qdrant`, and depending on the chosen one, there will be some subtle differences. The options include:\n",
    "- Local mode, no server required\n",
    "- Docker deployments\n",
    "- Qdrant Cloud\n",
    "\n",
    "See the [installation instructions](https://qdrant.tech/documentation/install/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e03e8460-8f32-4d1f-bb93-4f7636a476fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 2.20.0 requires huggingface-hub>=0.21.2, but you have huggingface-hub 0.20.3 which is incompatible.\n",
      "langchain-huggingface 0.0.3 requires huggingface-hub>=0.23.0, but you have huggingface-hub 0.20.3 which is incompatible.\n",
      "langchain-huggingface 0.0.3 requires tokenizers>=0.19.1, but you have tokenizers 0.15.2 which is incompatible.\n",
      "accelerate 0.33.0 requires huggingface-hub>=0.21.0, but you have huggingface-hub 0.20.3 which is incompatible.\n",
      "transformers 4.43.3 requires huggingface-hub<1.0,>=0.23.2, but you have huggingface-hub 0.20.3 which is incompatible.\n",
      "transformers 4.43.3 requires tokenizers<0.20,>=0.19, but you have tokenizers 0.15.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-qdrant 'qdrant-client[fastembed]'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eeead681",
   "metadata": {},
   "source": [
    "### Credentials\n",
    "\n",
    "There are no credentials needed to run the code in this notebook.\n",
    "\n",
    "## Initialization\n",
    "\n",
    "For these examples we will use embeddings from the `langchain_ollama` package since it is free to use.\n",
    "\n",
    "### Local mode\n",
    "\n",
    "Python client allows you to run the same code in local mode without running the Qdrant server. That's great for testing things out and debugging or storing just a small amount of vectors. The embeddings might be fully kept in memory or persisted on disk.\n",
    "\n",
    "#### In-memory\n",
    "\n",
    "For some testing scenarios and quick experiments, you may prefer to keep all the data in memory only, so it gets lost when the client is destroyed - usually at the end of your script/notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8429667e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-04T10:51:22.525091Z",
     "start_time": "2023-04-04T10:51:22.522015Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_qdrant import QdrantVectorStore\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "\n",
    "embedding_function = OllamaEmbeddings(model=\"llama3\")\n",
    "client = QdrantClient(\":memory:\")\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=\"demo_collection\",\n",
    "    vectors_config=VectorParams(size=4096, distance=Distance.COSINE),\n",
    ")\n",
    "\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"demo_collection\",\n",
    "    embedding=embedding_function,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59f0b954",
   "metadata": {},
   "source": [
    "#### On-disk storage\n",
    "\n",
    "Local mode, without using the Qdrant server, may also store your vectors on disk so they persist between runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "24b370e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-04T10:51:24.827567Z",
     "start_time": "2023-04-04T10:51:22.529080Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = QdrantClient(path=\"/tmp/local_qdrant\")\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=\"demo_collection\",\n",
    "    vectors_config=VectorParams(size=4096, distance=Distance.COSINE),\n",
    ")\n",
    "\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"demo_collection\",\n",
    "    embedding=embedding_function,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "749658ce",
   "metadata": {},
   "source": [
    "### On-premise server deployment\n",
    "\n",
    "No matter if you choose to launch Qdrant locally with [a Docker container](https://qdrant.tech/documentation/install/), or select a Kubernetes deployment with [the official Helm chart](https://github.com/qdrant/qdrant-helm), the way you're going to connect to such an instance will be identical. You'll need to provide a URL pointing to the service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91e7f5ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-04T10:51:24.832708Z",
     "start_time": "2023-04-04T10:51:24.829905Z"
    }
   },
   "outputs": [],
   "source": [
    "url = \"<---qdrant url here --->\"\n",
    "qdrant = QdrantVectorStore.from_documents(\n",
    "    docs,\n",
    "    embeddings,\n",
    "    url=url,\n",
    "    prefer_grpc=True,\n",
    "    collection_name=\"my_documents\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c9e21ce9",
   "metadata": {},
   "source": [
    "### Qdrant Cloud\n",
    "\n",
    "If you prefer not to keep yourself busy with managing the infrastructure, you can choose to set up a fully-managed Qdrant cluster on [Qdrant Cloud](https://cloud.qdrant.io/). There is a free forever 1GB cluster included for trying out. The main difference with using a managed version of Qdrant is that you'll need to provide an API key to secure your deployment from being accessed publicly. The value can also be set in a `QDRANT_API_KEY` environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcf88bdf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-04T10:51:24.837599Z",
     "start_time": "2023-04-04T10:51:24.834690Z"
    }
   },
   "outputs": [],
   "source": [
    "url = \"<---qdrant cloud cluster url here --->\"\n",
    "api_key = \"<---api key here--->\"\n",
    "qdrant = QdrantVectorStore.from_documents(\n",
    "    docs,\n",
    "    embeddings,\n",
    "    url=url,\n",
    "    prefer_grpc=True,\n",
    "    api_key=api_key,\n",
    "    collection_name=\"my_documents\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825c7903",
   "metadata": {},
   "source": [
    "## Using an existing collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f772575",
   "metadata": {},
   "source": [
    "To get an instance of `langchain_qdrant.Qdrant` without loading any new documents or texts, you can use the `Qdrant.from_existing_collection()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf7a6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant = QdrantVectorStore.from_existing_collection(\n",
    "    embeddings=embeddings,\n",
    "    collection_name=\"my_documents\",\n",
    "    url=\"http://localhost:6333\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cddef6e",
   "metadata": {},
   "source": [
    "## Manage vector store\n",
    "\n",
    "### Add items to vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7697a362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5dc00d153425475a82b39dd4cfcf17a0',\n",
       " '880fe06607934d64982bf8a3fb4124ce',\n",
       " 'e00da4acfdcb48f9968e70279b42dd2d']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "document_1 = Document(\n",
    "    page_content=\"foo\",\n",
    "    metadata={\"source\": \"https://example.com\"}\n",
    ")\n",
    "\n",
    "document_2 = Document(\n",
    "    page_content=\"bar\",\n",
    "    metadata={\"source\": \"https://example.com\"}\n",
    ")\n",
    "\n",
    "document_3 = Document(\n",
    "    page_content=\"baz\",\n",
    "    metadata={\"source\": \"https://example.com\"}\n",
    ")\n",
    "\n",
    "documents = [document_1, document_2, document_3]\n",
    "\n",
    "vector_store.add_documents(documents=documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd23102",
   "metadata": {},
   "source": [
    "### Delete items from vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "999cafcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.delete(ids=[\"e00da4acfdcb48f9968e70279b42dd2d\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f9215c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-04T09:27:29.920258Z",
     "start_time": "2023-04-04T09:27:29.913714Z"
    }
   },
   "source": [
    "## Query vector store\n",
    "\n",
    "Once your vector store has been created and the relevant documents have been added you will most likely wish to query it during the running of your chain or agent. \n",
    "\n",
    "### Query directly\n",
    "\n",
    "The simplest scenario for using Qdrant vector store is to perform a similarity search. Under the hood, our query will be encoded into vector embeddings and used to find similar documents in Qdrant collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a8c513ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-04T10:51:25.204469Z",
     "start_time": "2023-04-04T10:51:24.855618Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* bar [{'source': 'https://example.com', '_id': '880fe06607934d64982bf8a3fb4124ce', '_collection_name': 'demo_collection'}]\n"
     ]
    }
   ],
   "source": [
    "results = vector_store.similarity_search(query=\"thud\",k=1)\n",
    "for doc in results:\n",
    "    print(f\"* {doc.page_content} [{doc.metadata}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd93d85",
   "metadata": {},
   "source": [
    "`QdrantVectorStore` supports 3 modes for similarity searches. They can be configured using the `retrieval_mode` parameter when setting up the class.\n",
    "\n",
    "- Dense Vector Search(Default)\n",
    "- Sparse Vector Search\n",
    "- Hybrid Search\n",
    "\n",
    "### Dense Vector Search\n",
    "\n",
    "To search with only dense vectors,\n",
    "\n",
    "- The `retrieval_mode` parameter should be set to `RetrievalMode.DENSE`(default).\n",
    "- A [dense embeddings](https://python.langchain.com/v0.2/docs/integrations/text_embedding/) value should be provided to the `embedding` parameter.\n",
    "\n",
    "### Sparse Vector Search\n",
    "\n",
    "To search with only sparse vectors,\n",
    "\n",
    "- The `retrieval_mode` parameter should be set to `RetrievalMode.SPARSE`.\n",
    "- An implementation of the [`SparseEmbeddings`](https://github.com/langchain-ai/langchain/blob/master/libs/partners/qdrant/langchain_qdrant/sparse_embeddings.py) interface using any sparse embeddings provider has to be provided as value to the `sparse_embedding` parameter.\n",
    "\n",
    "The `langchain-qdrant` package provides a [FastEmbed](https://github.com/qdrant/fastembed) based implementation out of the box.\n",
    "\n",
    "### Hybrid Vector Search\n",
    "\n",
    "To perform a hybrid search using dense and sparse vectors with score fusion,\n",
    "\n",
    "- The `retrieval_mode` parameter should be set to `RetrievalMode.HYBRID`.\n",
    "- A [dense embeddings](https://python.langchain.com/v0.2/docs/integrations/text_embedding/) value should be provided to the `embedding` parameter.\n",
    "- An implementation of the [`SparseEmbeddings`](https://github.com/langchain-ai/langchain/blob/master/libs/partners/qdrant/langchain_qdrant/sparse_embeddings.py) interface using any sparse embeddings provider has to be provided as value to the `sparse_embedding` parameter.\n",
    "\n",
    "Note that if you've added documents with the `HYBRID` mode, you can switch to any retrieval mode when searching. Since both the dense and sparse vectors are available in the collection."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1bda9bf5",
   "metadata": {},
   "source": [
    "If you want to execute a similarity search and receive the corresponding scores you can run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cf772328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([Record(id='5dc00d153425475a82b39dd4cfcf17a0', payload={'page_content': 'foo', 'metadata': {'source': 'https://example.com'}}, vector=None, shard_key=None, order_value=None),\n",
       "  Record(id='880fe06607934d64982bf8a3fb4124ce', payload={'page_content': 'bar', 'metadata': {'source': 'https://example.com'}}, vector=None, shard_key=None, order_value=None)],\n",
       " None)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.scroll(collection_name=\"demo_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8804a21d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-04T10:51:25.631585Z",
     "start_time": "2023-04-04T10:51:25.227384Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* [SIM=0.521379] bar [{'source': 'https://example.com', '_id': '880fe06607934d64982bf8a3fb4124ce', '_collection_name': 'demo_collection'}]\n"
     ]
    }
   ],
   "source": [
    "results = vector_store.similarity_search_with_score(query=\"thud\",k=1)\n",
    "for doc, score in results:\n",
    "    print(f\"* [SIM={score:3f}] {doc.page_content} [{doc.metadata}]\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "525e3582",
   "metadata": {},
   "source": [
    "For a full list of all the search functions available for a `QdrantVectorStore`, read the [API reference](https://api.python.langchain.com/en/latest/vectorstores/langchain_qdrant.vectorstores.Qdrant.html)\n",
    "\n",
    "### Metadata filtering\n",
    "\n",
    "Qdrant has an [extensive filtering system](https://qdrant.tech/documentation/concepts/filtering/) with rich type support. It is also possible to use the filters in Langchain, by passing an additional param to both the `similarity_search_with_score` and `similarity_search` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dc7cffc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* foo [{'source': 'https://example.com', '_id': '5dc00d153425475a82b39dd4cfcf17a0', '_collection_name': 'demo_collection'}]\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client.http import models\n",
    "\n",
    "results = vector_store.similarity_search(query=\"thud\",k=1,filter=models.Filter(should=[\n",
    "            models.FieldCondition(\n",
    "                key=\"page_content\",\n",
    "                match=models.MatchValue(value=\"foo\"),\n",
    "            ),\n",
    "        ]\n",
    "))\n",
    "for doc in results:\n",
    "    print(f\"* {doc.page_content} [{doc.metadata}]\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "691a82d6",
   "metadata": {},
   "source": [
    "### Query by turning into retriever\n",
    "\n",
    "You can also transform the vector store into a retriever for easier usage in your chains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9427195f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-04T10:51:26.031451Z",
     "start_time": "2023-04-04T10:51:26.018763Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://example.com', '_id': '880fe06607934d64982bf8a3fb4124ce', '_collection_name': 'demo_collection'}, page_content='bar')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\": 1}\n",
    ")\n",
    "retriever.invoke(\"thud\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c851b4f",
   "metadata": {},
   "source": [
    "Using retriever in a simple RAG chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "64348f1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-04T10:51:26.043909Z",
     "start_time": "2023-04-04T10:51:26.034284Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know.\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"thud\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0358ecde",
   "metadata": {},
   "source": [
    "## Customizing Qdrant\n",
    "\n",
    "There are options to use an existing Qdrant collection within your Langchain application. In such cases, you may need to define how to map Qdrant point into the Langchain `Document`.\n",
    "\n",
    "### Named vectors\n",
    "\n",
    "Qdrant supports [multiple vectors per point](https://qdrant.tech/documentation/concepts/collections/#collection-with-multiple-vectors) by named vectors. If you work with a collection created externally or want to have the differently named vector used, you can configure it by providing its name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f11adf8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "QdrantVectorStore.from_documents(\n",
    "    docs,\n",
    "    embedding=embeddings,\n",
    "    sparse_embedding=sparse_embeddings,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"my_documents_2\",\n",
    "    retrieval_mode=RetrievalMode.HYBRID,\n",
    "    vector_name=\"custom_vector\",\n",
    "    sparse_vector_name=\"custom_sparse_vector\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2350093",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Metadata\n",
    "\n",
    "Qdrant stores your vector embeddings along with the optional JSON-like payload. Payloads are optional, but since LangChain assumes the embeddings are generated from the documents, we keep the context data, so you can extract the original texts as well.\n",
    "\n",
    "By default, your document is going to be stored in the following payload structure:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"page_content\": \"Lorem ipsum dolor sit amet\",\n",
    "    \"metadata\": {\n",
    "        \"foo\": \"bar\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "You can, however, decide to use different keys for the page content and metadata. That's useful if you already have a collection that you'd like to reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d6baf9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-04T11:08:31.739141Z",
     "start_time": "2023-04-04T11:08:30.229748Z"
    }
   },
   "outputs": [],
   "source": [
    "QdrantVectorStore.from_documents(\n",
    "    docs,\n",
    "    embeddings,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"my_documents_2\",\n",
    "    content_payload_key=\"my_page_content_key\",\n",
    "    metadata_payload_key=\"my_meta\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2300e785",
   "metadata": {},
   "source": [
    "## API reference\n",
    "\n",
    "For detailed documentation of all `QdrantVectorStore` features and configurations head to the API reference: https://api.python.langchain.com/en/latest/vectorstores/langchain_qdrant.vectorstores.Qdrant.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
