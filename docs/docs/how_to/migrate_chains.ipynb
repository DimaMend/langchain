{
 "cells": [
  {
   "cell_type": "raw",
   "id": "bc346658-6820-413a-bd8f-11bd3082fe43",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: How to migrate chains to LCEL\n",
    "---\n",
    "\n",
    "import { ColumnContainer, Column } from \"@theme/Columns\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919a5ae2-ed21-4923-b98f-723c111bac67",
   "metadata": {},
   "source": [
    "\n",
    ":::{.callout-tip} \n",
    "We recommend reading the LCEL [conceptual guide](/docs/concepts#langchain-expression-language-lcel) first.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f331037f-be3f-4782-856f-d55dab952488",
   "metadata": {},
   "source": [
    "LCEL is designed to streamline the process of building useful apps with LLMs and combining related components. It does this by providing:\n",
    "\n",
    "1. **A unified interface**: Every LCEL object implements the `Runnable` interface, which defines a common set of invocation methods (`invoke`, `batch`, `stream`, `ainvoke`, ...). This makes it possible for chains of LCEL objects to also automatically support useful operations like batching and streaming of intermediate steps, since every chain of LCEL objects is itself an LCEL object.\n",
    "2. **Composition primitives**: LCEL provides a number of primitives that make it easy to compose chains, parallelize components, add fallbacks, dynamically configure chain internals, and more.\n",
    "\n",
    "LangChain maintains a number of legacy abstractions. Many of these can be easily implemented via short combinations of LCEL primitives. Doing so confers some advantages:\n",
    "\n",
    "- The resulting chains typically implement the full `Runnable` interface, including streaming and asynchronous support where appropriate;\n",
    "- The chains may be more easily extended or modified;\n",
    "- The parameters of the chain are typically surfaced for easier customization (e.g., prompts).\n",
    "\n",
    "In this guide we review LCEL implementations of common legacy abstractions. Where appropriate, we link out to separate guides with more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99b47ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet  langchain-core langchain langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3621b62-a037-42b8-8faa-59575608bb8b",
   "metadata": {},
   "source": [
    "## LLMChain\n",
    "[LLMChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.llm.LLMChain.html) composes a prompt template, LLM, and output parser.\n",
    "\n",
    "<ColumnContainer>\n",
    "\n",
    "<Column>\n",
    "\n",
    "#### Legacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e628905c-430e-4e4a-9d7c-c91d2f42052e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"user\", \"Tell me a {adjective} joke\")],\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=ChatOpenAI(), prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc3b527-c09e-4c77-9711-c3cc4506cd95",
   "metadata": {},
   "source": [
    "\n",
    "</Column>\n",
    "\n",
    "<Column>\n",
    "\n",
    "#### LCEL\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d2a7cf8-1bc7-405c-bb0d-f2ab2ba3b6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"user\", \"Tell me a {adjective} joke\")],\n",
    ")\n",
    "\n",
    "chain = prompt | ChatOpenAI() | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0b0513-77b8-4371-a20e-3e487cec7e7f",
   "metadata": {},
   "source": [
    "\n",
    "</Column>\n",
    "</ColumnContainer>\n",
    "\n",
    "Note that `LLMChain` by default returns a `dict` containing both the input and the output. If this behavior is desired, we can replicate it using another LCEL primitive, [RunnablePassthrough](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.passthrough.RunnablePassthrough.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "529206c5-abbe-4213-9e6c-3b8586c8000d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "outer_chain = RunnablePassthrough().assign(text=chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d2e26c-2854-4971-9c2b-613450993921",
   "metadata": {},
   "source": [
    "See [this tutorial](/docs/tutorials/llm_chain) for more detail on building with prompt templates, LLMs, and output parsers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00df631d-5121-4918-94aa-b88acce9b769",
   "metadata": {},
   "source": [
    "## ConversationChain\n",
    "\n",
    "[ConversationChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.conversation.base.ConversationChain.html) incorporates a memory of previous messages to sustain a stateful conversation.\n",
    "\n",
    "\n",
    "<ColumnContainer>\n",
    "<Column>\n",
    "\n",
    "#### Legacy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f2cc6dc-d70a-4c13-9258-452f14290da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "chain = ConversationChain(llm=ChatOpenAI(), memory=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e36b0e-c7dc-4130-a51b-189d4b756c7f",
   "metadata": {},
   "source": [
    "</Column>\n",
    "\n",
    "<Column>\n",
    "\n",
    "#### LCEL\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "173e1a9c-2a18-4669-b0de-136f39197786",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "history = InMemoryChatMessageHistory()\n",
    "chain = RunnableWithMessageHistory(ChatOpenAI(), lambda x: history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b386ce6-895e-442c-88f3-7bec0ab9f401",
   "metadata": {},
   "source": [
    "See [this tutorial](/docs/tutorials/chatbot) for more detail on building with [RunnableWithMessageHistory](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html).\n",
    "\n",
    "Note that `ConversationChain` has no notion of threads, or separate sessions. `RunnableWithMessageHistory` implements this concept via configuration parameters. It should be instantiated with a callable that returns a [chat message history](https://api.python.langchain.com/en/latest/chat_history/langchain_core.chat_history.BaseChatMessageHistory.html). By default, it expects this function to take a single argument `session_id`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e05994f-1fbc-4699-bf2e-62cb0e4deeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "chain = RunnableWithMessageHistory(ChatOpenAI(), get_session_history)\n",
    "\n",
    "chain.invoke(\"Hello!\", config={\"configurable\": {\"session_id\": \"abc123\"}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b41e78-ddeb-44d0-a58b-a0ea0c99a761",
   "metadata": {},
   "source": [
    "</Column>\n",
    "</ColumnContainer>\n",
    "\n",
    "\n",
    "## ConversationRetrievalChain\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
