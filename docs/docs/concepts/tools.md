# Tools

## What is a Tool?

## Interface

LangChain tools implement the [BaseTool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.base.BaseTool.html#langchain_core.tools.base.BaseTool) interface.



Tools are utilities designed to be called by a model: their inputs are designed to be generated by models, and their outputs are designed to be passed back to models.
Tools are needed whenever you want a model to control parts of your code or call out to external APIs.

A tool consists of:

1. The `name` of the tool.
2. A `description` of what the tool does.
3. A `JSON schema` defining the inputs to the tool.
4. A `function` (and, optionally, an async variant of the function).

When a tool is bound to a model, the name, description and JSON schema are provided as context to the model.
Given a list of tools and a set of instructions, a model can request to call one or more tools with specific inputs.
Typical usage may look like the following:

```python
tools = [...] # Define a list of tools
llm_with_tools = llm.bind_tools(tools)
ai_msg = llm_with_tools.invoke("do xyz...")
# -> AIMessage(tool_calls=[ToolCall(...), ...], ...)
```

The `AIMessage` returned from the model MAY have `tool_calls` associated with it.
Read [this guide](/docs/concepts/#aimessage) for more information on what the response type may look like.

Once the chosen tools are invoked, the results can be passed back to the model so that it can complete whatever task
it's performing.
There are generally two different ways to invoke the tool and pass back the response:

#### Invoke with just the arguments

When you invoke a tool with just the arguments, you will get back the raw tool output (usually a string).
This generally looks like:

```python
# You will want to previously check that the LLM returned tool calls
tool_call = ai_msg.tool_calls[0]
# ToolCall(args={...}, id=..., ...)
tool_output = tool.invoke(tool_call["args"])
tool_message = ToolMessage(
    content=tool_output,
    tool_call_id=tool_call["id"],
    name=tool_call["name"]
)
```

Note that the `content` field will generally be passed back to the model.
If you do not want the raw tool response to be passed to the model, but you still want to keep it around,
you can transform the tool output but also pass it as an artifact (read more about [`ToolMessage.artifact` here](/docs/concepts/#toolmessage))

```python
... # Same code as above
response_for_llm = transform(response)
tool_message = ToolMessage(
    content=response_for_llm,
    tool_call_id=tool_call["id"],
    name=tool_call["name"],
    artifact=tool_output
)
```

#### Invoke with `ToolCall`

The other way to invoke a tool is to call it with the full `ToolCall` that was generated by the model.
When you do this, the tool will return a ToolMessage.
The benefits of this are that you don't have to write the logic yourself to transform the tool output into a ToolMessage.
This generally looks like:

```python
tool_call = ai_msg.tool_calls[0]
# -> ToolCall(args={...}, id=..., ...)
tool_message = tool.invoke(tool_call)
# -> ToolMessage(
#      content="tool result foobar...",
#      tool_call_id=...,
#      name="tool_name"
#    )
```

If you are invoking the tool this way and want to include an [artifact](/docs/concepts/#toolmessage) for the ToolMessage, you will need to have the tool return two things.
Read more about [defining tools that return artifacts here](/docs/how_to/tool_artifacts/).

#### Best practices

When designing tools to be used by a model, it is important to keep in mind that:

- Chat models that have explicit [tool-calling APIs](/docs/concepts/#functiontool-calling) will be better at tool calling than non-fine-tuned models.
- Models will perform better if the tools have well-chosen names, descriptions, and JSON schemas. This is another form of prompt engineering.
- Simple, narrowly scoped tools are easier for models to use than complex tools.

#### Related

For specifics on how to use tools, see the [tools how-to guides](/docs/how_to/#tools).

To use a pre-built tool, see the [tool integration docs](/docs/integrations/tools/).



## How to Create a Tool

## How to Use a Tool

## Related Resources

* [How-to Guides: Tools](https://python.langchain.com/docs/how_to/#tools)



https://python.langchain.com/docs/how_to/custom_tools/

----

Hereâ€™s a conceptual document based on your structure:

# Tools

## Overview
Tools are utilities that allow a model to perform specific tasks or interact with external systems. By providing well-defined inputs and outputs, tools enable models to call functions, interact with APIs, or control parts of your code. Tools are essential when you want to extend a model's capabilities beyond standard tasks.

A tool typically consists of:
- A `name`.
- A `description` that helps the model understand its purpose.
- A `JSON schema` defining the inputs required by the tool.
- A `function` (which may have an async version) to perform the task.

Tools are called by models when they detect that external functionality is needed to complete a task.

## Key Concepts

- **Tool Name**: A clear, descriptive identifier of what the tool does.
- **Description**: A concise explanation that the model can use to decide when to call the tool.
- **JSON Schema**: Defines the input parameters the tool expects, helping the model provide accurate arguments.
- **Function**: The underlying code that gets executed when the tool is invoked.
- **Tool Invocation**: The process by which a model calls a tool, providing the necessary arguments for it to function.
- **Tool Output**: The result returned by the tool, which may be passed back to the model to help it continue its task.
  
### Example
```python
tools = [my_tool] # Define a list of tools
llm_with_tools = llm.bind_tools(tools)
response = llm_with_tools.invoke("perform task with tool")
```

## Create a Tool

To create a tool, you need to implement the following:

1. **Define the Tool**: 
   - Provide a name and description.
   - Create a JSON schema for the input parameters.
   
2. **Implement the Function**:
   - Write the function that performs the desired task based on the inputs.
   - Optionally, create an async version if needed.

3. **Bind the Tool**:
   - Add the tool to a model so that it can call it when necessary.

### Example
```python
class MyTool(BaseTool):
    name = "my_tool"
    description = "A tool that does something"
    def run(self, args):
        return "Result of the tool"
```

## Use a Tool

After defining a tool, you can invoke it in different ways:

- **Invoke with Arguments**: The simplest way to call a tool is to pass just the arguments. The raw output of the tool is returned.
  
  ```python
  tool_output = tool.invoke(tool_call["args"])
  ```

- **Invoke with ToolCall**: This method involves passing the entire `ToolCall` object, which can handle both the input arguments and any additional logic needed.

  ```python
  tool_message = tool.invoke(tool_call)
  ```

In both cases, the output can be returned to the model for further processing.

## Use with Tool Calling

Certain models are explicitly fine-tuned for tool-calling APIs. These models can efficiently handle tools by interpreting their names, descriptions, and schemas more accurately. To use tools effectively with these models:
- Ensure that the tool's name and description are clear and concise.
- Design tools to be simple and focused on specific tasks to minimize errors in tool selection and invocation.

## Injected Arguments

Sometimes, you need to pass additional arguments or contextual information to tools at runtime. These arguments can be injected dynamically, depending on the task being performed by the model. This allows for more flexible tool usage based on the model's needs.

Example:
```python
tool_message = ToolMessage(
    content=tool_output,
    tool_call_id=tool_call["id"],
    name=tool_call["name"],
    additional_data=context_info
)
```

## Passing Run Time Information

In some cases, the model needs real-time data (e.g., API keys, timestamps) to execute a tool. You can pass this information at runtime when invoking the tool.

Example:
```python
tool_call = ai_msg.tool_calls[0]
tool_output = tool.invoke({
    "api_key": runtime_api_key,
    **tool_call["args"]
})
```

## Making Stateful Tools

Stateful tools maintain context across invocations, allowing the model to track information or decisions made in previous steps. To create stateful tools, you can store intermediate data and retrieve it during subsequent invocations.

### Example:
```python
class StatefulTool(BaseTool):
    def __init__(self):
        self.state = {}
    def run(self, args):
        # Modify and use state as needed
        return "Processed with state"
```

## Related Resources

- [How-to Guides: Tools](https://python.langchain.com/docs/how_to/#tools)
- [Tool Integration Docs](https://python.langchain.com/docs/integrations/tools)
- [Custom Tools Documentation](https://python.langchain.com/docs/how_to/custom_tools)