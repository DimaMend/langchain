# Multimodality

LLMs are models that operate on sequences of tokens to predict the next token in a sequence. Tokens are abstract representations of input data that can take a variety of forms, such as text, code, images, audio, video, and more.


Tokens are abstract representations of input data that can take a variety of forms, such as text, code, images, audio, video, and more.

The core technology powering these models, based on the [transformer architectures](https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture), operates on sequences of [tokens](/docs/concepts/tokens).

LLMs are trained to predict the next token in a sequence of tokens. Tokens are abstract representations of input data which can take a variety of forms, such as text, code, images, audio, video, but could represent even more abstract input such as DNA sequences, protein sequences, and more.


Some chat models are multimodal, accepting images, audio and even video as inputs. These are still less common, meaning model providers haven't standardized on the "best" way to define the API. Multimodal **outputs** are even less common. As such, we've kept our multimodal abstractions fairly light weight and plan to further solidify the multimodal APIs and interaction patterns as the field matures.

In LangChain, most chat models that support multimodal inputs also accept those values in OpenAI's content blocks format. So far this is restricted to image inputs. For models like Gemini which support video and other bytes input, the APIs also support the native, model-specific representations.

For specifics on how to use multimodal models, see the [relevant how-to guides here](/docs/how_to/#multimodal).

For a full list of LangChain model providers with multimodal models, [check out this table](/docs/integrations/chat/#advanced-features).


