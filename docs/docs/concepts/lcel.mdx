# LangChain Expression Language (LCEL)

:::info Prerequisites
* [Runnable Interface](/docs/concepts/runnables)
:::

The *L*ang*C*hain *E*xpression *L*anguage (LCEL) takes a [declarative](https://en.wikipedia.org/wiki/Declarative_programming) approach to building new [Runnables](/docs/concepts/runnables) from existing Runnables.

This means that you describe what you want to happen, rather than how you want it to happen, allowing LangChain to optimize the run-time execution of the chains.

We often refer to a `Runnable` created using LCEL as a "chain". It's important to remember that a "chain" is `Runnable` and it implements the full [Runnable Interface](/docs/concepts/runnables).

:::note
* Please see the following list of [how-to guides](/docs/how_to/#langchain-expression-language-lcel)
* A list of built-in `Runnables` can be found in the [LangChain Core API Reference](https://python.langchain.com/api_reference/core/runnables.html). Many of these Runnables are useful when composing custom "chains" in LangChain using LCEL.
* The [LCEL cheatsheet](https://python.langchain.com/docs/how_to/lcel_cheatsheet/) shows common patterns that involve the Runnable interface and LCEL expressions.
:::

## Benefits of LCEL

LangChain optimizes the run-time execution of chains built with LCEL in a number of ways:

- **Optimize parallel execution**: Run Runnables in parallel using [RunnableParallel](#RunnableParallel) or run multiple inputs through a given chain in parallel using the [Runnable Batch API](/docs/concepts/runnables#batch). Parallel execution can significantly reduce the latency as processing can be done in parallel instead of sequentially.
- **Guarantee Async support**: Any chain built with LCEL can be run asynchronously using the [Runnable Async API](/docs/concepts/runnables#async-api). This can be useful when running chains in a server environment where you want to handle large number of requests concurrently.
- **Minimize streaming latency**: LCEL chains can be streamed, allowing for incremental output as the chain is executed. LangChain can optimize the streaming of the output to minimize the time-to-first-token(time elapsed until the first chunk of output from a [chat model](/docs/concepts/chat-models) or [llm](/docs/concepts/llms) comes out).

Other benefits include:

- [**Seamless LangSmith tracing**](https://docs.smith.langchain.com)
As your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.
With LCEL, **all** steps are automatically logged to [LangSmith](https://docs.smith.langchain.com/) for maximum observability and debuggability.
- **Standard API**: Because all chains are built using the Runnable interface, they can be used in the same way as any other Runnable.
- [**Deployable with LangServe**](/docs/concepts/langserve): Chains built with LCEL can be deployed using for production use.


## Orchestration using LCEL

LCEL [orchestration solution](https://en.wikipedia.org/wiki/Orchestration_(computing)) -- it allows LangChain to handle run-time execution of chains in an optimized way.


TBD

However, LCEL is not necessarily the best solution for all use cases. It works perfectly for simple
applications that require chaining together a few Runnables, but for more complex applications that may involve
multiple agents, branching or complex state management, we'd recommend using [LangGraph](/docs/concepts/langgraph).

## Composition Primitives

`LCEL` chains are built by composing existing `Runnables` together. The two main composition primitives are [RunnableSequence](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableSequence.html#langchain_core.runnables.base.RunnableSequence) and [RunnableParallel](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableParallel.html#langchain_core.runnables.base.RunnableParallel).

Many other composition primitives (e.g., [RunnableAssign](
https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnableAssign.html#langchain_core.runnables.passthrough.RunnableAssign
)) can be thought of as variations of these two primitives.

:::note
You can find a list of all composition primitives in the [LangChain Core API Reference](https://python.langchain.com/api_reference/core/runnables.html).
:::

### RunnableSequence

`RunnableSequence` is a composition primitive that allows you "chain" multiple runnables sequentially, with the output of one runnable serving as the input to the next.

```python
from langchain_core.runnables import RunnableSequence
chain = RunnableSequence([runnable1, runnable2])
```

Invoking the `chain` with some input:

```python
final_output = chain.invoke(some_input)
```

corresponds to the following:

```python
output1 = runnable1.invoke(some_input)
final_output = runnable2.invoke(output1)
```

:::note
`runnable1` and `runnable2` are placeholders for any `Runnable` that you want to chain together.
:::

### RunnableParallel

`RunnableParallel` is a composition primitive that allows you to run multiple runnables concurrently, with the same input provided to each.

```python
from langchain_core.runnables import RunnableParallel
chain = RunnableParallel({
    "key1": runnable1,
    "key2": runnable2,
})
```

Invoking the `chain` with some input:

```python
final_output = chain.invoke(some_input)
```

Will yield a `final_output` dictionary with the same keys as the input dictionary, but with the values replaced by the output of the corresponding runnable.

```python
{
    "key1": runnable1.invoke(some_input),
    "key2": runnable2.invoke(some_input),
}
```

But of course under the hood, the runnables are executed in parallel, so while the result
is the same as dictionary comprehension shown above, the execution time is much faster.

`RunnableParallel` supports both `sync` and `async` runnables. The `sync` runnables are run in parallel using a `ThreadPoolExecutor`, while the `async` runnables are run concurrently using `asyncio`.

`RunnableParallel` uses a `ThreadPoolExecutor` to run the runnables concurrently when


:::note
`runnable1` and `runnable2` are placeholders for any `Runnable` that you want to run in parallel.
:::

### Shorthand Syntax

The usage of `RunnableSequence` and `RunnableParallel` is so common that that we've

LCEL provides a shorthand syntax for creating chains using `RunnableSequence` and `RunnableParallel`.

Th

###

One point about LangChain Expression Language is that any two runnables can be "chained" together into sequences. The output of the previous runnable's .invoke() call is passed as input to the next runnable. This can be done using the pipe operator (|), or the more explicit .pipe() method, which does the same thing.

The resulting RunnableSequence is itself a runnable, which means it can be invoked, streamed, or further chained just like any other runnable. Advantages of chaining runnables in this way are efficient streaming (the sequence will stream output as soon as it is available), and debugging and tracing with tools like LangSmith.

## Coercion

We can even combine this chain with more runnables to create another chain. This may involve some input/output formatting using other types of runnables, depending on the required inputs and outputs of the chain components.

For example, let's say we wanted to compose the joke generating chain with another chain that evaluates whether or not the generated joke was funny.

We would need to be careful with how we format the input into the next chain. In the below example, the dict in the chain is automatically parsed and converted into a RunnableParallel, which runs all of its values in parallel and returns a dict with the results.

### Composition Syntax

To make it easier to compose chains, LCEL provides a simple syntax for creating chains using `RunnableSequence` and `RunnableParallel`.

#### RunnableSequence

```python
from langchain_core.runnables import RunnableSequence

chain = RunnableSequence([
    runnable1,
    runnable2,
    runnable3,
])
```

Is equivalent to:

```python
chain = runnable1 | runnable2 | runnable3
```

#### RunnableParallel

https://python.langchain.com/docs/how_to/functions/

```python

from langchain_core.runnables import RunnableParallel

chain = RunnableParallel({
    "key1": runnable1,
    "key2": runnable2,
    "key3": runnable3,
})
```

Is equivalent to:

```python

chain = {
    "key1": runnable1,
    "key2": runnable2,
    "key3": runnable3,
} | another_runnable
```


## Legacy Chains

LCEL aims to provide consistency around behavior and customization over legacy subclassed chains such as `LLMChain` and
`ConversationalRetrievalChain`. Many of these legacy chains hide important details like prompts, and as a wider variety
of viable models emerge, customization has become more and more important.

If you are currently using one of these legacy chains, please see [this guide for guidance on how to migrate](/docs/versions/migrating_chains).

For guides on how to do specific tasks with LCEL, check out [the relevant how-to guides](/docs/how_to/#langchain-expression-language-lcel).