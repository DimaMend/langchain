# Tool Calling

## Motivation 

Many AI applications interact directly with humans (e.g., chatbots). In these cases, it is approrpiate for models to respond in natural langague. 

But what about cases where we want a model to also interact *directly* with external systems (e.g., a databases or an API)?

These systems often have a particular input schema (e.g., APIs have a required payload structure), making natural language inapporpiate for use.

This motivates the concept of tool calling:

* Tools (e.g., a databases or an API) can be bound to models.
* Models can respond in the input schema required to run the tool.

## Tool Definition

There are several ways to create tools in LangChain.

### Function-based tools

Implementation: 

```python
def my_tool(arg1: str, arg2: int) -> str:
    """Tool description"""
    # Implementation
```

Pros:
* Simplest syntax, lowest barrier to entry
* Clean and intuitive for Python developers

Cons:
* Less control over tool schema/metadata
* Docstring parsing may be inconsistent across different LangChain components
* Can't directly invoke as a tool for testing
* Can't easily check the parsed tool schema


### Function with @tool decorator

Implementation

```python
@tool
def my_tool(arg1: str, arg2: int) -> str:
    """Tool description"""
    # Implementation
```

Pros:
* More control over tool schema/metadata
* Consistent parsing across LangChain
* Can be directly invoked as a tool
* Can check parsed schema easily
* Supports additional configuration (e.g. "artifact and content")

Cons:
* Slightly more verbose than plain functions


### Runnable-based tools

Implementation

```python
from langchain.schema.runnable import Runnable

class MyTool(Runnable):
    # Implementation
``` 

Pros:
* Full control over execution logic
* Can implement complex tools with state

Cons:
* More verbose
* Requires understanding Runnable interface

### Subclassing BaseTool

Implementation

```python
from langchain.tools import BaseTool

class MyTool(BaseTool):
    # Implementation
```

Pros:
* Full control over tool behavior
* Can implement complex tools with state

Cons:
* Most verbose option
* Requires understanding BaseTool interface

### StructuredTool.from_function:

```python
from langchain.tools import StructuredTool

def my_func(arg1: str, arg2: int) -> str:
    # Implementation

my_tool = StructuredTool.from_function(my_func)
```

Pros:
* Combines simplicity of functions with more control
* Can add additional metadata
Cons:
* Less commonly used, may be unfamiliar to some users

### Overall considerations

* Models will perform better if the tools have well-chosen names, descriptions, and JSON schemas.
* Simple, narrowly scoped tools are easier for models to use than complex tools.
* For simple tools, plain functions or @tool decorator are often sufficient.
* For production code or complex tools, using @tool or a class-based approach provides more control and safety.
* Consistency in parsing and behavior across different LangChain components (e.g. bind_tools, ToolNode) is important.
* Testing and debugging capabilities (e.g. direct invocation, checking parsed schema) should be considered.
* Long-term, the goal may be to simplify and standardize tool creation, possibly through higher-level wrappers.

The choice between these methods often depends on the specific use case, the complexity of the tool, and the need for additional control or metadata.

See this [how-to guide](https://python.langchain.com/docs/how_to/custom_tools/) for more details!

## Tool Calling

:::info
You will sometimes hear the term `function calling`. We use this term interchangeably with `tool calling`. 
:::

When using tools, we need to specify two things:

* A model that supports tool calling.
* Tools that the model can call.

#### Model

While tool calling is not universal, many model providers support it.

See our [model integration page](/docs/integrations/chat/) for a list of models that support tool calling.

LangChain provides a standardized interface for tool calling that is consistent across different models.

`model.bind_tools()` is a method for specifying which tools are available for a model to call. 

If a model has been initialized as `model` without tools, we can bind tools to it as a list:

```python
model_with_tools = model.bind_tools([tool])
```

#### Tool

Of course, need to specify a `tool` that the model can call.

As discussed above, there are many ways to create a tool. 

In the simplest case, any function can be bound as a tool to a model that supports tool calling.

```python
def multiply(a: int, b: int) -> int:
    """Multiply a and b.

    Args:
        a: first int
        b: second int
    """
    return a * b

llm_with_tools = llm.bind_tools([multiply])
```

For more details on usage, see our [how-to guide focused on creating tools](docs/how_to/#tools)!

In addition to user-defined functions, LangChain also offers many toolkit [integrations](/docs/integrations/tools/) that can be used off-the-shelf.

## Invoking 

![Diagram of a tool call by a model](/img/tool_call_example.png)

If we pass an input that is relevant to the tool - e.g., `What is 2 multiplied by 3` - the model will return a tool call.

```python
result = llm_with_tools.invoke("What is 2 multiplied by 3?")
```

The tool call has specific arguments needed to run the tool, as explained below.

## Tool Call Output

It is important to note that the model only generates the *arguments* to invoke the tool. 

Actually running the tool is up to the user or application (downstream of the model).

As an exaple, the output `result` from the above invocation will be an `AIMessage`. 

If the tool was called, the `result` will have a `tool_calls` attribute.

This attribute has the tool name, arguments, and id:

```
result.tool_calls
{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'xxx', 'type': 'tool_call'}
```



For more details on usage, see our [how-to guides](docs/how_to/#tools)!

## Tool execution

With the tool call argument returned by an LLM, we can now execute the tool.

In the simplest case, the arguments can be easily extracted from the AI message. 

If the tool is simply a function, as defined above, the arguments can be passed to the tool directly:

```python
args = tool_call.tool_calls[0]['args']
result = multiply(**args)
```

TODO: Add discussion and example of how to execute tools that are not simple functions.

https://python.langchain.com/docs/how_to/#tools

## Common patterns  

We saw that a model can call a tool, returning the payload needed to invoke the tool.

We saw the arguments can be easily extracted from this payload and passed to the tool. 

What if the tool output is *passed back* to the model?

![Diagram of a tool calling agent](/img/tool_calling_agent.png)

This loop can be repeated, allowing a model to call multiple (potentially different) tools in sequence to perform a task.

This is the intuition behind ReAct, a general purpose tool-calling agent architecture. 

* Act - let the model call specific tools
* Observe - pass the tool output back to the model
* Reason - let the model reason about the tool output to decide what to do next (e.g., call another tool or just respond directly)

This general purpose architecture can be applied to many types of tools.

See our detailed [how-to guide focused on tool calling agents](https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/) for more details!

## Best practices

When designing tools to be used by a model, it is important to keep in mind that:

* Models that have explicit [tool-calling APIs](/docs/concepts/#functiontool-calling) will be better at tool calling than non-fine-tuned models.
* Models will perform better if the tools have well-chosen names and descriptions.
* Simple, narrowly scoped tools are easier for models to use than complex tools.
* Asking the model to select from a large list of tools poses challenges for the model.