{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86fc5bb2-017f-434e-8cd6-53ab214a5604",
   "metadata": {},
   "source": [
    "# Добавление истории сообщений\n",
    "\n",
    "Как правило пользователь хочет обмениваться сообщениями с вопросно-ответными приложениями.\n",
    "В таком случае приложению нужна «память» о предыдущих вопросах и ответах и логика для включения этих данных в текущие рассуждения.\n",
    "\n",
    "В этом разделе показано как добавить историю сообщений в приложение, разработанное в разделе [Быстрый старт](/docs/use_cases/question_answering/quickstart).\n",
    "\n",
    ":::note\n",
    "\n",
    "Подробнее об истории сообщений — в разделе [Работа с историей сообщений](/docs/expression_language/how_to/message_history).\n",
    "\n",
    ":::\n",
    "\n",
    "Для добавления истории сообщений в приложение потребуется:\n",
    "\n",
    "1. Доработать промпт, чтобы он поддерживал историю сообщений на входе.\n",
    "2. Добавить цепочку, которая берет последний вопрос пользователя и переформулирует его в контексте истории чата. Это нужно, если последний вопрос ссылается на контекст предыдущих сообщений. Например, если пользователь задает уточняющий вопрос: «Расскажи подробнее о втором пункте?». На такой вопрос нельзя ответить без предыдущего контекста."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487d8d79-5ee9-4aa4-9fdf-cd5f4303e099",
   "metadata": {},
   "source": [
    "## Установка зависимостей\n",
    "\n",
    "Для разработки используются модели генерации и эмбеддингов GigaChat, а также векторное хранилище FAISS.\n",
    "Вы можете использовать любую [векторное хранилище](/docs/modules/data_connection/vectorstores/) или [ретривер](/docs/modules/data_connection/retrievers/)\n",
    "\n",
    "Установите пакеты с помощью команды:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d272cd-4e31-40aa-bbb4-0be0a1f49a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet  gigachain langchainhub faiss-cpu bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6ba684-26cf-4860-904e-a4d51380c134",
   "metadata": {},
   "source": [
    "## Пример цепочки без истории сообщений\n",
    "\n",
    "Пример цепочки без поддержки истории сообщений, разобранный в разделе [Быстрый старт](/docs/use_cases/question_answering/quickstart):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8a913b1-0eea-442a-8a64-ec73333f104b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.chat_models.gigachat import GigaChat\n",
    "from langchain_community.embeddings.gigachat import GigaChatEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "820244ae-74b4-4593-b392-822979dd91b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка, разделение на части и индексация содержимого блога.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=GigaChatEmbeddings(credentials=\"<авторизационные_данные>\", verify_ssl_certs=False))\n",
    "\n",
    "# Извлечение данных и генерация с помощью релевантных фрагментов блога.\n",
    "retriever = vectorstore.as_retriever()\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "llm = GigaChat(credentials=\"<авторизационные_данные>\", verify_ssl_certs=False)\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22206dfd-d673-4fa4-887f-349d273cb3f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task Decomposition is the process of breaking down a complicated task into smaller and simpler steps. This allows an agent to better understand and plan ahead for the task. CoT (Chain of Thought) and Tree of Thoughts are techniques that help enhance model performance on complex tasks by decomposing them into manageable steps. Task decomposition can be done using LLMs with simple prompting, task-specific instructions, or with human inputs.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776ae958-cbdc-4471-8669-c6087436f0b5",
   "metadata": {},
   "source": [
    "## Добавление контекста к вопросу\n",
    "\n",
    "Сначала нужно определить цепочку, которая:\n",
    "\n",
    "* получает на вход историю сообщений и последний вопрос пользователя;\n",
    "* переформулирует вопрос, если он ссылается на данные, доступные в истории сообщений.\n",
    "\n",
    "Добавим промпт, который содержит переменную `MessagesPlaceholder` под названием `chat_history`.\n",
    "С помощью ключа `chat_history` в промпт можно передать историю сообщений, которая будет помещена после системного промпта и перед последним вопросом пользователя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b685428-8b82-4af1-be4f-7232c5d55b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "which might reference context in the chat history, formulate a standalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cbd8d7-7162-4fb0-9e69-67ea4d4603a5",
   "metadata": {},
   "source": [
    "Полученная цепочка переформулирует вопрос, который ссылается на предыдущие данные, и, таким образом позволяет обращаться к истории сообщений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46ee9aa1-16f1-4509-8dae-f8c71f4ad47d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The term \"large\" in this context refers to the size of the model, which is typically much larger than standard language models.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "contextualize_q_chain.invoke(\n",
    "    {\n",
    "        \"chat_history\": [\n",
    "            HumanMessage(content=\"What does LLM stand for?\"),\n",
    "            AIMessage(content=\"Large language model\"),\n",
    "        ],\n",
    "        \"question\": \"What is meant by large\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a47168-4a1f-4e39-bd2d-d5b03609a243",
   "metadata": {},
   "source": [
    "## Цепочка с историей чата\n",
    "\n",
    "Теперь можно собрать итоговую вопросно-ответную логику.\n",
    "\n",
    "В ней цепочка `contextualize_q_chain` выполняет только при наличии истории сообщений.\n",
    "Это возможно благодаря тому, что если функция в LCEL-цепочке возвращает другую цепочку, она также будет вызвана."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66f275f3-ddef-4678-b90d-ee64576878f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "Use three sentences maximum and keep the answer concise.\\\n",
    "\n",
    "{context}\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51fd0e54-5bb4-4a9a-b012-87a18ebe2bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Common ways of doing Task Decomposition include using Chain of Thought (CoT), which instructs the model to think step by step and utilize more test-time computation to decompose hard tasks into smaller and simpler steps.', response_metadata={'token_usage': Usage(prompt_tokens=703, completion_tokens=49, total_tokens=752), 'model_name': 'GigaChat:3.1.24.3', 'finish_reason': 'stop'})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "question = \"What is Task Decomposition?\"\n",
    "ai_msg_1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=question), ai_msg_1[\"answer\"]])\n",
    "\n",
    "second_question = \"What are common ways of doing it?\"\n",
    "ai_msg_2 = rag_chain.invoke({\"input\": second_question, \"chat_history\": chat_history})\n",
    "\n",
    "print(ai_msg_2[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf6c7e0-84f8-4747-b2ae-e84315152bd9",
   "metadata": {},
   "source": [
    "Настоящее вопросно-ответное приложение должно уметь сохранять историю сообщений, обновлять ее и использовать в контексте.\n",
    "\n",
    "Такую функциональность можно добавить с помощью классов:\n",
    "\n",
    "- [BaseChatMessageHistory](/docs/modules/memory/chat_messages/) — хранение истории сообщений.\n",
    "- [RunnableWithMessageHistory](/docs/expression_language/how_to/message_history) — обертка для LCEL-цепочки и `BaseChatMessageHistory`, которая преобразует историю сообщений в вводные данные и обновляет ее после каждого вызова.\n",
    "\n",
    "Подробно о том как использовать эти классы для разработки разговорной цепочки — в разделе [Работа с историей сообщений](/docs/expression_language/how_to/message_history).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
