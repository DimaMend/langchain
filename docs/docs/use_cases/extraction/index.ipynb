{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e397959-1622-4c1c-bdb6-4660a3c39e14",
   "metadata": {},
   "source": [
    "# Извлечение структурированной информации\n",
    "\n",
    "Большие языковые модели (LLM) отлично подходят для решения задачи по извлечению информации.\n",
    "\n",
    "Обычно, такие задачи решаются вручную, с помощью множества различных регулярных выражений и специально настроенных ML-моделей.\n",
    "Такие решения требуют больших трудозатрат при обслуживании и доработке.\n",
    "\n",
    "В то же время LLM для извлечения определенной информации достаточно предоставить подходящие иснтрукции и несколько примеров.\n",
    "\n",
    "## Способы извлечения информации\n",
    "\n",
    "Можно выделить три подхода к извлечению  информации с помощью LLM:\n",
    "\n",
    "* Использование инструментов или функций.\n",
    "\n",
    "  Некоторые модели поддерживают работу с инструментами или функциями. Такие LLM могут структурировать извлеченную информацию в соответствии с предоставленной схемой данных.\n",
    "  В общем случае этот подход наиболее прост в разработке и позволяет получить хорошие результаты.\n",
    "\n",
    "* Генерирование JSON.\n",
    "  \n",
    "  Некоторые модели могут принудительно возвращать данные в виде JSON.\n",
    "  Такой подход отличается от работы с инструментами или функциями тем, что формат вывода задается как часть промпта.\n",
    "\n",
    "* Использование промпта.\n",
    "\n",
    "  Если какая-то модель хорошо следует инструкциям, то ей можно указать вернуть данные в определеном формате.\n",
    "  После чего сгенерированный текст можно преобразовать в структурированные данные вроде JSON с помощью [встроенного](/docs/modules/model_io/output_parsers/) или [собственного](/docs/modules/model_io/output_parsers/custom) парсера.\n",
    "  Этот подход можно использовать если модель не поддерживает принудительную генерацию JSON или не умеет работать с инструментами или функциями.\n",
    "  Его можно применять с более широким набором моделей, но итоговые результаты, как правило будут хуже.\n",
    "\n",
    "## Быстрый старт\n",
    "\n",
    "В [быстром старте](/docs/use_cases/extraction/quickstart) приводится готовый пример, демонстрирующий извлечение информации с помощью GigaChat на основе работы с инструментами/функциями. \n",
    "\n",
    "## Руководства \n",
    "\n",
    "* Используйте [образцы кода](/docs/use_cases/extraction/how_to/examples) для более эффективного извлечения информации.\n",
    "* Узнайте [что делать](/docs/use_cases/extraction/how_to/handle_long_text), когда размер текста превышает объем контекста модели.\n",
    "* Изучите как использовать загрузчики документов и парсеры GigaChain для [извлечения данных из файлов](/docs/use_cases/extraction/how_to/handle_files) вроде PDF.\n",
    "* Ознакомьтесь с [примером получения структурированной информации](/docs/use_cases/extraction/how_to/parse) от моделей, которые не поддерживают работу с инструментами или функциями. \n",
    "\n",
    "## Рекомендации\n",
    "\n",
    "Раздел [Рекомендации](/docs/use_cases/extraction/guidelines) содержит список советов о том, как добиться лучших результатов при извлечении информации.\n",
    "\n",
    "<!--\n",
    "## Use Case Accelerant\n",
    "\n",
    "[langchain-extract](https://github.com/langchain-ai/langchain-extract) is a starter repo that implements a simple web server for information extraction from text and files using LLMs. It is build using **FastAPI**, **LangChain** and **Postgresql**. Feel free to adapt it to your own use cases.\n",
    "-->\n",
    "\n",
    "## Смотрите также\n",
    "\n",
    "* Документация для парсеров [выходных данных](/docs/modules/model_io/output_parsers/), которая включает примеры парсеров данныз разных типов: списков, даты и времени, перечислений и других.\n",
    "* Документация для [загрузчиков документов](/docs/modules/data_connection/document_loaders/)\n",
    "\n",
    "<!--\n",
    "* The experimental [Anthropic function calling](/docs/integrations/chat/anthropic_functions) support provides similar functionality to Anthropic chat models.\n",
    "* [LlamaCPP](/docs/integrations/llms/llamacpp#grammars) natively supports constrained decoding using custom grammars, making it easy to output structured content using local LLMs \n",
    "* [JSONFormer](/docs/integrations/llms/jsonformer_experimental) offers another way for structured decoding of a subset of the JSON Schema.\n",
    "* [Kor](https://eyurtsev.github.io/kor/) is another library for extraction where schema and examples can be provided to the LLM. Kor is optimized to work for a parsing approach.\n",
    "* [OpenAI's function and tool calling](https://platform.openai.com/docs/guides/function-calling)\n",
    "* For example, see [OpenAI's JSON mode](https://platform.openai.com/docs/guides/text-generation/json-mode).\n",
    "-->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
