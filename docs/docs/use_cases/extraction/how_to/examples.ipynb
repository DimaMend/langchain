{
 "cells": [
  {
   "cell_type": "raw",
   "id": "a37d08e8-8d6d-4cf2-8215-2aafb6877fb5",
   "metadata": {},
   "source": [
    "---\n",
    "title: Use Reference Examples\n",
    "sidebar_position: 1\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70403d4f-50c1-43f8-a7ea-a211167649a5",
   "metadata": {},
   "source": [
    "The quality of extractions can often be improved by providing reference examples to the LLM.\n",
    "\n",
    ":::{.callout-tip}\n",
    "While this tutorial focuses how to use examples with a tool calling model, this technique is generally applicable, and will work\n",
    "also with JSON more or prompt based techniques.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89579144-bcb3-490a-8036-86a0a6bcd56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# Определяем промпт: добавляем инструкций и доп. контекст\n",
    "# Здесь мы можем:\n",
    "# 1) Добавить примеров работы функций, для улучшения качества извлечения информации\n",
    "# 2) Предоставить дополнительное информации из чего и что вы будете извлекать информацию\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Ты эксперт в извлечении информации из текста. \"\n",
    "            \"Извлекай только релевантную информацию из текста. \"\n",
    "            \"Если ты не знаешь значение аттрибута, \"\n",
    "            \"который нужно извлечь, поставь аттрибуту null.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(\"examples\"),  # <---- Примеры работы\n",
    "        (\"human\", \"{text}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2484008c-ba1a-42a5-87a1-628a900de7fd",
   "metadata": {},
   "source": [
    "Test out the template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "610c3025-ea63-4cd7-88bd-c8cbcb4d8a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "ChatPromptValue(messages=[SystemMessage(content='Ты эксперт в извлечении информации из текста. Извлекай только релевантную информацию из текста. Если ты не знаешь значение аттрибута, который нужно извлечь, поставь аттрибуту null.'), HumanMessage(content='testing 1 2 3'), HumanMessage(content='this is some text')])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import (\n",
    "    HumanMessage,\n",
    ")\n",
    "\n",
    "prompt.invoke(\n",
    "    {\"text\": \"this is some text\", \"examples\": [HumanMessage(content=\"testing 1 2 3\")]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368abd80-0cf0-41a7-8224-acf90dd6830d",
   "metadata": {},
   "source": [
    "## Define the schema\n",
    "\n",
    "Let's re-use the person schema from the quickstart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d875a49a-d2cb-4b9e-b5bf-41073bc3905c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class Person(BaseModel):\n",
    "    \"\"\"Информация о человека.\"\"\"\n",
    "\n",
    "    # ^ Док-строка выше, подкладывается в описании функции\n",
    "    # и может помочь в улучшении результатов от LLM\n",
    "\n",
    "    # Заметьте:\n",
    "    # 1. Каждое поле опциональное -- это позволяет LLM не извлекать поля, которые не описаны\n",
    "    # 2. Каждое поле имеет поле description — это подкладывается в описание аргументов функции\n",
    "    # и может помочь в улучшении результатов\n",
    "    name: Optional[str] = Field(..., description=\"Имя человека\")\n",
    "    hair_color: Optional[str] = Field(\n",
    "        ..., description=\"Цвет волос человека, заполни если известен\"\n",
    "    )\n",
    "    height_in_meters: Optional[float] = Field(\n",
    "        ..., description=\"Высота человека в метрах.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class Data(BaseModel):\n",
    "    \"\"\"Информация о людях.\"\"\"\n",
    "\n",
    "    # Создаем модель, чтобы мы могли извлечь информацию о нескольких людях\n",
    "    people: List[Person]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c42162-e4f6-4461-88fd-c76f5aab7e32",
   "metadata": {},
   "source": [
    "## Define reference examples\n",
    "\n",
    "Examples can be defined as a list of input-output pairs. \n",
    "\n",
    "Each example contains an example `input` text and an example `output` showing what should be extracted from the text.\n",
    "\n",
    ":::{.callout-important}\n",
    "This is a bit in the weeds, so feel free to ignore if you don't get it!\n",
    "\n",
    "The format of the example needs to match the API used (e.g., tool calling or JSON mode etc.).\n",
    "\n",
    "Here, the formatted examples will match the format expected for the tool calling API since that's what we're using.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08356810-77ce-4e68-99d9-faa0326f2cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from typing import List, TypedDict\n",
    "\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    BaseMessage,\n",
    "    FunctionMessage,\n",
    "    HumanMessage,\n",
    ")\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "\n",
    "\n",
    "class Example(TypedDict):\n",
    "    \"\"\"Пример работы функций.\"\"\"\n",
    "\n",
    "    input: str  # Пример вызова\n",
    "    function_calls: List[BaseModel]  # Pydantic модель, с примером извлечения\n",
    "    function_outputs: List[str]\n",
    "\n",
    "\n",
    "def tool_example_to_messages(example: Example) -> List[BaseMessage]:\n",
    "    \"\"\"Превращаем примеры вызовов функций в историю сообщений\"\"\"\n",
    "    messages: List[BaseMessage] = [HumanMessage(content=example[\"input\"])]\n",
    "    for function_call, function_output in itertools.zip_longest(\n",
    "        example[\"function_calls\"], example.get(\"function_outputs\", [])\n",
    "    ):\n",
    "        messages.append(\n",
    "            AIMessage(\n",
    "                content=\"\",\n",
    "                additional_kwargs={\n",
    "                    \"function_call\": {\n",
    "                        # Названием функции в текущий момент соответствует The name of the function right now corresponds\n",
    "                        # to the name of the pydantic model\n",
    "                        # This is implicit in the API right now,\n",
    "                        # and will be improved over time.\n",
    "                        \"name\": function_call.__class__.__name__,\n",
    "                        \"arguments\": function_call.dict(),\n",
    "                    },\n",
    "                },\n",
    "            )\n",
    "        )\n",
    "        output = \"You have correctly called this tool.\"\n",
    "        if function_output:\n",
    "            output = function_output\n",
    "        messages.append(\n",
    "            FunctionMessage(name=function_call.__class__.__name__, content=output)\n",
    "        )\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463aa282-51c4-42bf-9463-6ca3b2c08de6",
   "metadata": {},
   "source": [
    "Next let's define our examples and then convert them into message format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f59a745-5c81-4011-a4c5-a33ec1eca7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    (\n",
    "        \"Океан огромный и синий. Глубина более 20 000 футов. В нем много рыбы.\",\n",
    "        Data(people=[]),\n",
    "    ),\n",
    "    (\n",
    "        \"Фиона отправилась в путешествие из Испании во Францию\",\n",
    "        Data(people=[Person(name=\"Фиона\", height_in_meters=None, hair_color=None)]),\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "messages = []\n",
    "\n",
    "for text, function_call in examples:\n",
    "    messages.extend(\n",
    "        tool_example_to_messages({\"input\": text, \"function_calls\": [function_call]})\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdbda30-e7e3-46b5-a54a-1769c580af93",
   "metadata": {},
   "source": [
    "Let's test out the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e61fa3a5-3d15-46a2-a23b-788f9a3ede52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "ChatPromptValue(messages=[SystemMessage(content='Ты эксперт в извлечении информации из текста. Извлекай только релевантную информацию из текста. Если ты не знаешь значение аттрибута, который нужно извлечь, поставь аттрибуту null.'), HumanMessage(content='Океан огромный и синий. Глубина более 20 000 футов. В нем много рыбы.'), AIMessage(content='', additional_kwargs={'function_call': {'name': 'Data', 'arguments': {'people': []}}}), FunctionMessage(content='You have correctly called this tool.', name='Data'), HumanMessage(content='Фиона отправилась в путешествие из Испании во Францию'), AIMessage(content='', additional_kwargs={'function_call': {'name': 'Data', 'arguments': {'people': [{'name': 'Фиона', 'hair_color': None, 'height_in_meters': None}]}}}), FunctionMessage(content='You have correctly called this tool.', name='Data'), HumanMessage(content='this is some text')])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.invoke({\"text\": \"this is some text\", \"examples\": messages})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b0bbef-bc6b-4535-a8e2-5c84f09d5637",
   "metadata": {},
   "source": [
    "## Create an extractor\n",
    "Here, we'll create an extractor using **gpt-4**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbfea43d-769b-42e9-a76f-ce722f7d6f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models.gigachat import GigaChat\n",
    "\n",
    "llm = GigaChat(\n",
    "    verify_ssl_certs=False,\n",
    "    timeout=6000,\n",
    "    model=\"GigaChat-Pro\",\n",
    "    temperature=0.01,\n",
    ")\n",
    "\n",
    "\n",
    "runnable = prompt | llm.with_structured_output(\n",
    "    schema=Data,\n",
    "    include_raw=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a8139e-f201-4b8e-baf0-16a83e5fa987",
   "metadata": {},
   "source": [
    "## Without examples 😿\n",
    "\n",
    "Notice that even though we're using gpt-4, it's failing with a **very simple** test case!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b1d6273-5ec5-4970-af8a-0da1f1efa293",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Giga generation stopped with reason: function_call\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people=[Person(name='Земля', hair_color='синий', height_in_meters=None)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Giga generation stopped with reason: function_call\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people=[Person(name='Земля', hair_color='синий', height_in_meters=None)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Giga generation stopped with reason: function_call\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people=[Person(name='Земля', hair_color='синий', height_in_meters=None)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Giga generation stopped with reason: function_call\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people=[Person(name='Земля', hair_color='синий', height_in_meters=None)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Giga generation stopped with reason: function_call\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people=[Person(name='Земля', hair_color='синий', height_in_meters=None)]\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    text = \"Если бы у Земли были бы волосы, они были бы синими\"\n",
    "    print(runnable.invoke({\"text\": text, \"examples\": []}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09840f17-ab26-4ea2-8a39-c747103804ec",
   "metadata": {},
   "source": [
    "## With examples 😻\n",
    "\n",
    "Reference examples helps to fix the failure!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bdfa49e-0005-4c06-9598-2adfd882b014",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Giga generation stopped with reason: function_call\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Giga generation stopped with reason: function_call\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Giga generation stopped with reason: function_call\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Giga generation stopped with reason: function_call\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Giga generation stopped with reason: function_call\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people=[]\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    text = \"Если бы у Земли были бы волосы, они были бы синими\"\n",
    "    print(runnable.invoke({\"text\": text, \"examples\": messages}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84413e17-608d-4f85-b70e-00b89b271927",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Giga generation stopped with reason: function_call\n"
     ]
    },
    {
     "data": {
      "text/plain": "Data(people=[Person(name='Джо', hair_color='синий', height_in_meters=None)])"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable.invoke(\n",
    "    {\n",
    "        \"text\": \"Моё имя Джо, мои волосы синие\",\n",
    "        \"examples\": messages,\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
